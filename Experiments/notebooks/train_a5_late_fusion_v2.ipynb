{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.5 Late Fusion (Frozen RGB + Frozen Depth + Trainable Fusion)\n",
    "\n",
    "**Experiment:** A.5  \n",
    "**Architecture:** Late Fusion (Two-Stream with Feature Fusion)  \n",
    "**Input:** RGB (3-channel) + Depth (3-channel) - processed separately  \n",
    "**Objective:** Combine RGB and Depth features for improved detection  \n",
    "**Classes:** 1 (fresh_fruit_bunch)\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "RGB Image (3ch)          Depth Image (3ch)\n",
    "     |                        |\n",
    "     v                        v\n",
    "[Frozen RGB Backbone]    [Frozen Depth Backbone]\n",
    "  (from A.1 weights)       (from A.2 weights)\n",
    "     |                        |\n",
    "     +-----> P3 Features <----+\n",
    "              (256 ch each)\n",
    "                   |\n",
    "                   v\n",
    "         [Concatenate: 512 ch]\n",
    "                   |\n",
    "                   v\n",
    "         [1x1 Conv: 512 -> 256]\n",
    "         [BatchNorm + SiLU]\n",
    "                   |\n",
    "                   v\n",
    "         [YOLO Detection Head]\n",
    "              (trainable)\n",
    "                   |\n",
    "              [Output]\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "- **Frozen Backbones:** RGB (A.1) and Depth (A.2) backbones are 100% frozen\n",
    "- **Trainable Components:** Only fusion layer (1x1 Conv) and detection head\n",
    "- **Dual Input:** Separate RGB and Depth images loaded together\n",
    "- **Memory Note:** Batch size 8 due to dual backbone forward pass\n",
    "\n",
    "## Uniform Augmentation (All Experiments)\n",
    "- translate: 0.1\n",
    "- scale: 0.5\n",
    "- fliplr: 0.5\n",
    "- hsv_h: 0.0 (disabled)\n",
    "- hsv_s: 0.0 (disabled)\n",
    "- hsv_v: 0.0 (disabled)\n",
    "- erasing: 0.0\n",
    "- mosaic: 0.0\n",
    "- mixup: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Environment Setup & Install\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    BASE_PATH = Path('/kaggle/working')\n",
    "else:\n",
    "    BASE_PATH = Path(r'D:/Work/Assisten Dosen/Anylabel/Experiments')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q ultralytics\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"A.5 LATE FUSION - ENVIRONMENT SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Base Path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Imports\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from copy import deepcopy\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils.loss import v8DetectionLoss\n",
    "from ultralytics.utils.ops import xywh2xyxy\n",
    "\n",
    "# Disable wandb logging\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Configuration - UNIFORM AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "# Uniform augmentation parameters (MUST match A.1-A.4b)\n",
    "AUGMENT_PARAMS = {\n",
    "    'translate': 0.1,\n",
    "    'scale': 0.5,\n",
    "    'fliplr': 0.5,\n",
    "    'hsv_h': 0.0,      # Disabled for uniformity\n",
    "    'hsv_s': 0.0,      # Disabled for uniformity\n",
    "    'hsv_v': 0.0,      # Disabled for uniformity\n",
    "    'erasing': 0.0,\n",
    "    'mosaic': 0.0,     # Disabled for uniformity\n",
    "    'mixup': 0.0,\n",
    "    'degrees': 0.0,\n",
    "    'copy_paste': 0.0,\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "SEEDS = [42, 123, 456, 789, 101]\n",
    "EXP_PREFIX = \"exp_a5_fusion\"\n",
    "EPOCHS = 100\n",
    "PATIENCE = 30\n",
    "IMGSZ = 640\n",
    "BATCH_SIZE = 8  # Smaller due to dual backbone (memory intensive)\n",
    "DEVICE = 0 if torch.cuda.is_available() else 'cpu'\n",
    "NUM_WORKERS = 4 if not IS_KAGGLE else 2\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"A.5 LATE FUSION - TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Experiment:   A.5 Late Fusion\")\n",
    "print(f\"Seeds:        {SEEDS} ({len(SEEDS)} runs)\")\n",
    "print(f\"Epochs:       {EPOCHS} (patience: {PATIENCE})\")\n",
    "print(f\"Image Size:   {IMGSZ}\")\n",
    "print(f\"Batch Size:   {BATCH_SIZE} (reduced for dual backbone)\")\n",
    "print(f\"Device:       {DEVICE}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  RGB Backbone:   FROZEN (from A.1)\")\n",
    "print(f\"  Depth Backbone: FROZEN (from A.2)\")\n",
    "print(f\"  Trainable:      Fusion Layer + Detection Head\")\n",
    "print(f\"\\nUniform Augmentation:\")\n",
    "for k, v in AUGMENT_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Paths Configuration\n",
    "# =============================================================================\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle paths - adjust dataset names as needed\n",
    "    RGB_DATASET = Path('/kaggle/input/ffb-localization-dataset/ffb_localization')\n",
    "    DEPTH_DATASET = Path('/kaggle/input/ffb-localization-depth')\n",
    "    # Pre-trained weights from A.1 and A.2 (upload as datasets)\n",
    "    RGB_WEIGHTS_DIR = Path('/kaggle/input/ffb-a1-weights')\n",
    "    DEPTH_WEIGHTS_DIR = Path('/kaggle/input/ffb-a2-weights')\n",
    "else:\n",
    "    # Local paths\n",
    "    RGB_DATASET = BASE_PATH / 'datasets' / 'ffb_localization'\n",
    "    DEPTH_DATASET = BASE_PATH / 'datasets' / 'ffb_localization_depth'\n",
    "    RGB_WEIGHTS_DIR = BASE_PATH / 'runs' / 'detect'\n",
    "    DEPTH_WEIGHTS_DIR = BASE_PATH / 'runs' / 'detect'\n",
    "\n",
    "RUNS_PATH = BASE_PATH / 'runs' / 'detect'\n",
    "KAGGLE_OUTPUT = BASE_PATH / 'kaggleoutput'\n",
    "RUNS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "KAGGLE_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Paths Configuration:\")\n",
    "print(f\"  RGB Dataset:     {RGB_DATASET}\")\n",
    "print(f\"  Depth Dataset:   {DEPTH_DATASET}\")\n",
    "print(f\"  RGB Weights Dir: {RGB_WEIGHTS_DIR}\")\n",
    "print(f\"  Depth Weights:   {DEPTH_WEIGHTS_DIR}\")\n",
    "print(f\"  Runs Path:       {RUNS_PATH}\")\n",
    "print(f\"  Output Path:     {KAGGLE_OUTPUT}\")\n",
    "\n",
    "# Verify datasets exist\n",
    "print(f\"\\nDataset Verification:\")\n",
    "for name, path in [('RGB', RGB_DATASET), ('Depth', DEPTH_DATASET)]:\n",
    "    exists = path.exists()\n",
    "    print(f\"  {name}: {'OK' if exists else 'NOT FOUND'} - {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Dual-Input Dataset Class\n",
    "# =============================================================================\n",
    "\n",
    "class LateFusionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that loads RGB and Depth images separately for late fusion.\n",
    "    Applies synchronized geometric augmentation to both modalities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        rgb_img_dir: Path,\n",
    "        depth_img_dir: Path,\n",
    "        label_dir: Path,\n",
    "        img_size: int = 640,\n",
    "        augment: bool = False,\n",
    "        augment_params: dict = None\n",
    "    ):\n",
    "        self.rgb_img_dir = Path(rgb_img_dir)\n",
    "        self.depth_img_dir = Path(depth_img_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.augment_params = augment_params or {}\n",
    "        \n",
    "        # Get list of files (use RGB as reference)\n",
    "        self.image_files = sorted([p.name for p in self.rgb_img_dir.glob('*.png')])\n",
    "        \n",
    "        # Filter to only include files that exist in both RGB and Depth\n",
    "        valid_files = []\n",
    "        for fname in self.image_files:\n",
    "            rgb_exists = (self.rgb_img_dir / fname).exists()\n",
    "            depth_exists = (self.depth_img_dir / fname).exists()\n",
    "            label_exists = (self.label_dir / fname.replace('.png', '.txt')).exists()\n",
    "            if rgb_exists and depth_exists and label_exists:\n",
    "                valid_files.append(fname)\n",
    "        \n",
    "        self.image_files = valid_files\n",
    "        print(f\"[LateFusionDataset] Loaded {len(self)} valid samples\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def _apply_augmentation(self, rgb, depth, labels):\n",
    "        \"\"\"\n",
    "        Apply synchronized geometric augmentation to RGB and Depth.\n",
    "        Only geometric transforms (translate, scale, fliplr) are applied.\n",
    "        \"\"\"\n",
    "        h, w = rgb.shape[:2]\n",
    "        \n",
    "        # Horizontal flip\n",
    "        if random.random() < self.augment_params.get('fliplr', 0.0):\n",
    "            rgb = cv2.flip(rgb, 1)\n",
    "            depth = cv2.flip(depth, 1)\n",
    "            if len(labels) > 0:\n",
    "                labels[:, 1] = 1.0 - labels[:, 1]  # Flip x_center\n",
    "        \n",
    "        # Scale and translate (affine transform)\n",
    "        scale = self.augment_params.get('scale', 0.0)\n",
    "        translate = self.augment_params.get('translate', 0.0)\n",
    "        \n",
    "        if scale > 0 or translate > 0:\n",
    "            # Random scale factor\n",
    "            s = random.uniform(1 - scale, 1 + scale)\n",
    "            \n",
    "            # Random translation\n",
    "            tx = random.uniform(-translate, translate) * w\n",
    "            ty = random.uniform(-translate, translate) * h\n",
    "            \n",
    "            # Affine matrix\n",
    "            M = np.array([\n",
    "                [s, 0, tx + (1 - s) * w / 2],\n",
    "                [0, s, ty + (1 - s) * h / 2]\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            # Apply to both images\n",
    "            rgb = cv2.warpAffine(rgb, M, (w, h), borderValue=(114, 114, 114))\n",
    "            depth = cv2.warpAffine(depth, M, (w, h), borderValue=(0, 0, 0))\n",
    "            \n",
    "            # Transform labels\n",
    "            if len(labels) > 0:\n",
    "                # Convert to pixel coordinates\n",
    "                x_center = labels[:, 1] * w\n",
    "                y_center = labels[:, 2] * h\n",
    "                box_w = labels[:, 3] * w\n",
    "                box_h = labels[:, 4] * h\n",
    "                \n",
    "                # Apply transformation\n",
    "                x_center = x_center * s + tx + (1 - s) * w / 2\n",
    "                y_center = y_center * s + ty + (1 - s) * h / 2\n",
    "                box_w = box_w * s\n",
    "                box_h = box_h * s\n",
    "                \n",
    "                # Convert back to normalized\n",
    "                labels[:, 1] = x_center / w\n",
    "                labels[:, 2] = y_center / h\n",
    "                labels[:, 3] = box_w / w\n",
    "                labels[:, 4] = box_h / h\n",
    "                \n",
    "                # Clip to valid range\n",
    "                labels[:, 1:] = np.clip(labels[:, 1:], 0, 1)\n",
    "                \n",
    "                # Filter out invalid boxes\n",
    "                valid = (labels[:, 3] > 0.001) & (labels[:, 4] > 0.001)\n",
    "                labels = labels[valid]\n",
    "        \n",
    "        return rgb, depth, labels\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Get a single sample with RGB, Depth, and labels.\n",
    "        \"\"\"\n",
    "        fname = self.image_files[idx]\n",
    "        \n",
    "        # Load RGB (BGR format from cv2)\n",
    "        rgb_path = self.rgb_img_dir / fname\n",
    "        rgb = cv2.imread(str(rgb_path))\n",
    "        \n",
    "        # Load Depth (3-channel from processed depth)\n",
    "        depth_path = self.depth_img_dir / fname\n",
    "        depth = cv2.imread(str(depth_path))\n",
    "        \n",
    "        if depth is None:\n",
    "            # Fallback: load as grayscale and convert to 3-channel\n",
    "            depth = cv2.imread(str(depth_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if depth is not None:\n",
    "                depth = cv2.cvtColor(depth, cv2.COLOR_GRAY2BGR)\n",
    "            else:\n",
    "                # Last fallback: use zeros\n",
    "                depth = np.zeros_like(rgb)\n",
    "        \n",
    "        # Load labels (YOLO format: class x_center y_center width height)\n",
    "        label_path = self.label_dir / fname.replace('.png', '.txt')\n",
    "        if label_path.exists():\n",
    "            labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n",
    "            if labels.size == 0:\n",
    "                labels = np.zeros((0, 5), dtype=np.float32)\n",
    "        else:\n",
    "            labels = np.zeros((0, 5), dtype=np.float32)\n",
    "        \n",
    "        # Resize if needed\n",
    "        if rgb.shape[:2] != (self.img_size, self.img_size):\n",
    "            rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n",
    "            depth = cv2.resize(depth, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Apply augmentation (synchronized)\n",
    "        if self.augment:\n",
    "            rgb, depth, labels = self._apply_augmentation(rgb, depth, labels)\n",
    "        \n",
    "        # Convert BGR to RGB for model input\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        depth = cv2.cvtColor(depth, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Normalize to [0, 1] and convert to tensor\n",
    "        rgb_tensor = torch.from_numpy(rgb).permute(2, 0, 1).float() / 255.0\n",
    "        depth_tensor = torch.from_numpy(depth).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        # Labels tensor\n",
    "        labels_tensor = torch.from_numpy(labels).float()\n",
    "        \n",
    "        return {\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'batch_idx': torch.zeros(len(labels)),\n",
    "            'img_path': str(rgb_path),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable-length labels.\"\"\"\n",
    "    rgb = torch.stack([item['rgb'] for item in batch])\n",
    "    depth = torch.stack([item['depth'] for item in batch])\n",
    "    \n",
    "    # Handle labels with batch index\n",
    "    labels_list = []\n",
    "    for i, item in enumerate(batch):\n",
    "        labels = item['labels']\n",
    "        if len(labels) > 0:\n",
    "            batch_idx = torch.full((len(labels), 1), i, dtype=torch.float32)\n",
    "            labels_with_idx = torch.cat([batch_idx, labels], dim=1)\n",
    "            labels_list.append(labels_with_idx)\n",
    "    \n",
    "    if labels_list:\n",
    "        labels = torch.cat(labels_list, dim=0)\n",
    "    else:\n",
    "        labels = torch.zeros((0, 6), dtype=torch.float32)\n",
    "    \n",
    "    return {\n",
    "        'rgb': rgb,\n",
    "        'depth': depth,\n",
    "        'labels': labels,\n",
    "        'img_paths': [item['img_path'] for item in batch],\n",
    "    }\n",
    "\n",
    "print(\"LateFusionDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 6: Late Fusion Model Architecture (Multi-Scale)\n# =============================================================================\n\nclass LateFusionModel(nn.Module):\n    \"\"\"\n    Late Fusion Model for FFB Detection with Multi-Scale Features.\n    \n    Architecture:\n        1. Frozen RGB backbone (from A.1) - extracts P3, P4, P5\n        2. Frozen Depth backbone (from A.2) - extracts P3, P4, P5\n        3. Concatenate features at each scale:\n           - P3: 128 + 128 = 256 -> 128 channels\n           - P4: 256 + 256 = 512 -> 256 channels  \n           - P5: 256 + 256 = 512 -> 256 channels\n        4. YOLO Detection head (trainable) on fused features\n    \"\"\"\n    \n    def __init__(\n        self,\n        rgb_model_path: str,\n        depth_model_path: str,\n        num_classes: int = 1,\n        device: str = 'cuda'\n    ):\n        super().__init__()\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"Initializing Late Fusion Model (Multi-Scale)\")\n        print(\"=\"*60)\n        \n        self.device = device\n        self.num_classes = num_classes\n        \n        # Load RGB model (A.1) and freeze\n        print(f\"\\nLoading RGB backbone from: {rgb_model_path}\")\n        self.rgb_yolo = YOLO(rgb_model_path)\n        self.rgb_backbone = self.rgb_yolo.model.model[0]  # Backbone only\n        for param in self.rgb_backbone.parameters():\n            param.requires_grad = False\n        self.rgb_backbone.eval()\n        print(f\"  RGB backbone frozen\")\n        \n        # Load Depth model (A.2) and freeze\n        print(f\"\\nLoading Depth backbone from: {depth_model_path}\")\n        self.depth_yolo = YOLO(depth_model_path)\n        self.depth_backbone = self.depth_yolo.model.model[0]  # Backbone only\n        for param in self.depth_backbone.parameters():\n            param.requires_grad = False\n        self.depth_backbone.eval()\n        print(f\"  Depth backbone frozen\")\n        \n        # Fusion layers for each scale\n        # P3: 128 (RGB) + 128 (Depth) = 256 -> 128\n        # P4: 256 (RGB) + 256 (Depth) = 512 -> 256\n        # P5: 256 (RGB) + 256 (Depth) = 512 -> 256\n        self.fusion_p3 = nn.Sequential(\n            nn.Conv2d(256, 128, kernel_size=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.SiLU(inplace=True)\n        )\n        self.fusion_p4 = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.SiLU(inplace=True)\n        )\n        self.fusion_p5 = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.SiLU(inplace=True)\n        )\n        \n        # Initialize fusion layers\n        for m in [self.fusion_p3, self.fusion_p4, self.fusion_p5]:\n            for layer in m.modules():\n                if isinstance(layer, nn.Conv2d):\n                    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n        \n        print(f\"\\nFusion layers initialized:\")\n        print(f\"  P3: 256 -> 128 channels\")\n        print(f\"  P4: 512 -> 256 channels\")\n        print(f\"  P5: 512 -> 256 channels\")\n        \n        # Detection head from RGB model (will be retrained)\n        # Copy the Detect head which expects multi-scale features\n        self.detect = deepcopy(self.rgb_yolo.model.model[-1])\n        for param in self.detect.parameters():\n            param.requires_grad = True\n        print(f\"\\nDetection head initialized (trainable)\")\n        \n        # Count parameters\n        self._count_parameters()\n    \n    def _count_parameters(self):\n        \"\"\"Count total and trainable parameters.\"\"\"\n        total = sum(p.numel() for p in self.parameters())\n        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        frozen = total - trainable\n        \n        print(f\"\\n[Parameter Count]\")\n        print(f\"  Total:      {total:,}\")\n        print(f\"  Trainable:  {trainable:,} ({100*trainable/total:.1f}%)\")\n        print(f\"  Frozen:     {frozen:,} ({100*frozen/total:.1f}%)\")\n        print(\"=\"*60)\n    \n    def _extract_features(self, backbone, x):\n        \"\"\"\n        Extract multi-scale features (P3, P4, P5) from backbone.\n        \n        For YOLOv11n:\n        - P3: stride 8, 128 channels (layer 4)\n        - P4: stride 16, 256 channels (layer 6)\n        - P5: stride 32, 256 channels (layer 8)\n        \"\"\"\n        features = []\n        extract_layers = [4, 6, 8]  # P3, P4, P5\n        \n        for i, module in enumerate(backbone):\n            x = module(x)\n            if i in extract_layers:\n                features.append(x)\n                if len(features) == len(extract_layers):\n                    break\n        \n        return features  # [P3, P4, P5]\n    \n    def forward(self, rgb: torch.Tensor, depth: torch.Tensor):\n        \"\"\"\n        Forward pass through late fusion model.\n        \n        Args:\n            rgb: RGB input [B, 3, H, W]\n            depth: Depth input [B, 3, H, W]\n            \n        Returns:\n            Detection output from YOLO Detect head\n        \"\"\"\n        # Extract multi-scale features from frozen backbones (no grad)\n        with torch.no_grad():\n            rgb_features = self._extract_features(self.rgb_backbone, rgb)   # [P3, P4, P5]\n            depth_features = self._extract_features(self.depth_backbone, depth)  # [P3, P4, P5]\n        \n        # Fuse each scale\n        # P3: 128 + 128 = 256 -> 128\n        # P4: 256 + 256 = 512 -> 256\n        # P5: 256 + 256 = 512 -> 256\n        fused_p3 = self.fusion_p3(torch.cat([rgb_features[0], depth_features[0]], dim=1))\n        fused_p4 = self.fusion_p4(torch.cat([rgb_features[1], depth_features[1]], dim=1))\n        fused_p5 = self.fusion_p5(torch.cat([rgb_features[2], depth_features[2]], dim=1))\n        \n        fused_features = [fused_p3, fused_p4, fused_p5]\n        \n        # Pass through detection head\n        output = self.detect(fused_features)\n        \n        return output\n    \n    def train(self, mode=True):\n        \"\"\"Set training mode, keeping backbones in eval.\"\"\"\n        super().train(mode)\n        # Always keep backbones in eval mode\n        self.rgb_backbone.eval()\n        self.depth_backbone.eval()\n        return self\n\n\nprint(\"LateFusionModel class defined with multi-scale features\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 7: Late Fusion Trainer with Proper YOLO Loss\n# =============================================================================\n\nclass LateFusionTrainer:\n    \"\"\"\n    Proper trainer for Late Fusion model using Ultralytics YOLO loss.\n    \n    Uses v8DetectionLoss for proper training with box_loss, cls_loss, and dfl_loss.\n    \"\"\"\n    \n    def __init__(\n        self,\n        model: nn.Module,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        device: str = 'cuda',\n        epochs: int = 100,\n        patience: int = 30,\n        lr: float = 0.01,\n        momentum: float = 0.937,\n        weight_decay: float = 0.0005,\n        save_dir: Path = None,\n        num_classes: int = 1,\n    ):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.epochs = epochs\n        self.patience = patience\n        self.save_dir = Path(save_dir) if save_dir else Path('runs/fusion')\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.num_classes = num_classes\n        \n        # Only optimize trainable parameters\n        trainable_params = [p for p in model.parameters() if p.requires_grad]\n        self.optimizer = torch.optim.SGD(\n            trainable_params,\n            lr=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            nesterov=True,\n        )\n        \n        # Learning rate scheduler - cosine annealing\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=epochs,\n            eta_min=lr * 0.01,\n        )\n        \n        # Mixed precision training\n        self.scaler = GradScaler()\n        \n        # Initialize YOLO loss function\n        # We need to create a dummy DetectionModel to get the loss function\n        self.criterion = self._create_loss_function()\n        \n        # Training state\n        self.best_fitness = 0.0\n        self.epochs_no_improve = 0\n        self.history = []\n        \n        # EMA (Exponential Moving Average) for model weights\n        self.ema = None\n        \n    def _create_loss_function(self):\n        \"\"\"\n        Create YOLO v8DetectionLoss function.\n        \n        Returns:\n            v8DetectionLoss instance\n        \"\"\"\n        from ultralytics.utils.loss import v8DetectionLoss\n        from ultralytics.nn.tasks import DetectionModel\n        \n        # Create a minimal DetectionModel to get the loss function\n        # The loss function needs access to model.stride and model.nc\n        class DummyDetectionModel:\n            def __init__(self, model):\n                self.model = model\n                self.nc = model.num_classes\n                # Get stride from the detect head if available\n                if hasattr(model, 'detect') and model.detect is not None:\n                    self.stride = model.detect.stride\n                else:\n                    # Default strides for YOLOv11n\n                    self.stride = torch.tensor([8., 16., 32.])\n        \n        dummy_model = DummyDetectionModel(self.model)\n        return v8DetectionLoss(dummy_model)\n    \n    def _prepare_batch(self, batch: Dict) -> Dict:\n        \"\"\"\n        Prepare batch for YOLO loss computation.\n        \n        Args:\n            batch: Batch from dataloader\n            \n        Returns:\n            Prepared batch dict with keys needed by v8DetectionLoss\n        \"\"\"\n        rgb = batch['rgb'].to(self.device)\n        depth = batch['depth'].to(self.device)\n        labels = batch['labels'].to(self.device)  # [batch_idx, class, x, y, w, h]\n        \n        # Split labels into components\n        if len(labels) > 0:\n            batch_idx = labels[:, 0].long()\n            cls = labels[:, 1].long()\n            bboxes = labels[:, 2:6]  # xywh normalized\n        else:\n            batch_idx = torch.zeros(0, dtype=torch.long, device=self.device)\n            cls = torch.zeros(0, dtype=torch.long, device=self.device)\n            bboxes = torch.zeros(0, 4, device=self.device)\n        \n        return {\n            'img': (rgb, depth),\n            'batch_idx': batch_idx,\n            'cls': cls,\n            'bboxes': bboxes,\n        }\n    \n    def train_epoch(self, epoch: int):\n        \"\"\"Train for one epoch.\"\"\"\n        self.model.train()\n        \n        # Ensure backbones stay in eval mode (frozen)\n        self.model.rgb_backbone.eval()\n        self.model.depth_backbone.eval()\n        \n        epoch_loss = 0.0\n        epoch_box_loss = 0.0\n        epoch_cls_loss = 0.0\n        epoch_dfl_loss = 0.0\n        \n        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n        \n        for batch_idx, batch in enumerate(pbar):\n            # Prepare batch\n            prepared_batch = self._prepare_batch(batch)\n            \n            self.optimizer.zero_grad()\n            \n            # Forward pass with autocast for mixed precision\n            with autocast():\n                rgb = prepared_batch['img'][0]\n                depth = prepared_batch['img'][1]\n                \n                # Forward through model\n                preds = self.model(rgb, depth)\n                \n                # Compute YOLO loss\n                loss, loss_items = self.criterion(preds, prepared_batch)\n                \n                # loss_items: [box_loss, cls_loss, dfl_loss]\n                box_loss = loss_items[0] if len(loss_items) > 0 else 0\n                cls_loss = loss_items[1] if len(loss_items) > 1 else 0\n                dfl_loss = loss_items[2] if len(loss_items) > 2 else 0\n            \n            # Backward pass\n            self.scaler.scale(loss).backward()\n            \n            # Gradient clipping\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)\n            \n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            # Track losses\n            epoch_loss += loss.item()\n            epoch_box_loss += box_loss.item() if torch.is_tensor(box_loss) else box_loss\n            epoch_cls_loss += cls_loss.item() if torch.is_tensor(cls_loss) else cls_loss\n            epoch_dfl_loss += dfl_loss.item() if torch.is_tensor(dfl_loss) else dfl_loss\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'box': f\"{box_loss.item() if torch.is_tensor(box_loss) else box_loss:.4f}\",\n                'cls': f\"{cls_loss.item() if torch.is_tensor(cls_loss) else cls_loss:.4f}\",\n            })\n        \n        num_batches = len(self.train_loader)\n        return {\n            'loss': epoch_loss / num_batches,\n            'box_loss': epoch_box_loss / num_batches,\n            'cls_loss': epoch_cls_loss / num_batches,\n            'dfl_loss': epoch_dfl_loss / num_batches,\n        }\n    \n    @torch.no_grad()\n    def validate(self):\n        \"\"\"Validate the model.\"\"\"\n        self.model.eval()\n        \n        val_loss = 0.0\n        val_box_loss = 0.0\n        val_cls_loss = 0.0\n        val_dfl_loss = 0.0\n        \n        for batch in self.val_loader:\n            prepared_batch = self._prepare_batch(batch)\n            \n            rgb = prepared_batch['img'][0]\n            depth = prepared_batch['img'][1]\n            \n            # Forward pass\n            preds = self.model(rgb, depth)\n            \n            # Compute loss\n            loss, loss_items = self.criterion(preds, prepared_batch)\n            \n            val_loss += loss.item()\n            val_box_loss += loss_items[0].item() if len(loss_items) > 0 and torch.is_tensor(loss_items[0]) else 0\n            val_cls_loss += loss_items[1].item() if len(loss_items) > 1 and torch.is_tensor(loss_items[1]) else 0\n            val_dfl_loss += loss_items[2].item() if len(loss_items) > 2 and torch.is_tensor(loss_items[2]) else 0\n        \n        num_batches = len(self.val_loader)\n        return {\n            'loss': val_loss / num_batches,\n            'box_loss': val_box_loss / num_batches,\n            'cls_loss': val_cls_loss / num_batches,\n            'dfl_loss': val_dfl_loss / num_batches,\n        }\n    \n    def save_checkpoint(self, epoch: int, is_best: bool = False):\n        \"\"\"Save model checkpoint.\"\"\"\n        weights_dir = self.save_dir / 'weights'\n        weights_dir.mkdir(exist_ok=True)\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'best_fitness': self.best_fitness,\n        }\n        \n        # Save last\n        torch.save(checkpoint, weights_dir / 'last.pt')\n        \n        if is_best:\n            torch.save(checkpoint, weights_dir / 'best.pt')\n            print(f\"  Saved best model (fitness: {self.best_fitness:.4f})\")\n    \n    def train(self):\n        \"\"\"Full training loop.\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"Starting Late Fusion Training\")\n        print(f\"{'='*60}\")\n        print(f\"Epochs: {self.epochs}\")\n        print(f\"Patience: {self.patience}\")\n        print(f\"Save directory: {self.save_dir}\")\n        print(f\"Device: {self.device}\")\n        print(f\"{'='*60}\\n\")\n        \n        for epoch in range(self.epochs):\n            # Train\n            train_metrics = self.train_epoch(epoch)\n            \n            # Validate\n            val_metrics = self.validate()\n            \n            # Update LR\n            self.scheduler.step()\n            \n            # Fitness: weighted combination of metrics (higher is better)\n            # Similar to Ultralytics: mAP50 with some weighting\n            fitness = 1.0 / (val_metrics['loss'] + 1e-6)\n            \n            # Check if best\n            is_best = fitness > self.best_fitness\n            if is_best:\n                self.best_fitness = fitness\n                self.epochs_no_improve = 0\n            else:\n                self.epochs_no_improve += 1\n            \n            # Save\n            self.save_checkpoint(epoch, is_best)\n            \n            # Log\n            self.history.append({\n                'epoch': epoch + 1,\n                'train_loss': train_metrics['loss'],\n                'train_box_loss': train_metrics['box_loss'],\n                'train_cls_loss': train_metrics['cls_loss'],\n                'train_dfl_loss': train_metrics['dfl_loss'],\n                'val_loss': val_metrics['loss'],\n                'val_box_loss': val_metrics['box_loss'],\n                'val_cls_loss': val_metrics['cls_loss'],\n                'val_dfl_loss': val_metrics['dfl_loss'],\n                'lr': self.optimizer.param_groups[0]['lr'],\n                'fitness': fitness,\n            })\n            \n            print(f\"\\nEpoch {epoch+1}/{self.epochs}:\")\n            print(f\"  Train Loss: {train_metrics['loss']:.4f} \"\n                  f\"(box: {train_metrics['box_loss']:.4f}, \"\n                  f\"cls: {train_metrics['cls_loss']:.4f}, \"\n                  f\"dfl: {train_metrics['dfl_loss']:.4f})\")\n            print(f\"  Val Loss:   {val_metrics['loss']:.4f} \"\n                  f\"(box: {val_metrics['box_loss']:.4f}, \"\n                  f\"cls: {val_metrics['cls_loss']:.4f}, \"\n                  f\"dfl: {val_metrics['dfl_loss']:.4f})\")\n            print(f\"  Fitness:    {fitness:.4f} {'*' if is_best else ''}\")\n            print(f\"  LR:         {self.optimizer.param_groups[0]['lr']:.6f}\")\n            \n            # Early stopping\n            if self.epochs_no_improve >= self.patience:\n                print(f\"\\n{'='*60}\")\n                print(f\"Early stopping at epoch {epoch+1}\")\n                print(f\"Best fitness: {self.best_fitness:.4f}\")\n                print(f\"{'='*60}\")\n                break\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Training complete!\")\n        print(f\"Best fitness: {self.best_fitness:.4f}\")\n        print(f\"{'='*60}\")\n        \n        return self.history\n\n\nprint(\"LateFusionTrainer class defined with proper YOLO loss\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Find Best Weights from A.1 and A.2\n",
    "# =============================================================================\n",
    "\n",
    "def find_best_weights(weights_dir: Path, exp_prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the best weights file from experiment runs.\n",
    "    Searches for exp_prefix_seed*/weights/best.pt\n",
    "    \"\"\"\n",
    "    # Try different patterns\n",
    "    patterns = [\n",
    "        f\"{exp_prefix}_seed*/weights/best.pt\",\n",
    "        f\"{exp_prefix}*/weights/best.pt\",\n",
    "        f\"*{exp_prefix}*/weights/best.pt\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = list(weights_dir.glob(pattern))\n",
    "        if matches:\n",
    "            # Return the first match (seed 42 is usually first)\n",
    "            return str(sorted(matches)[0])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Find RGB weights (A.1)\n",
    "rgb_weights = find_best_weights(RGB_WEIGHTS_DIR, 'exp_a1_rgb')\n",
    "if rgb_weights is None:\n",
    "    rgb_weights = find_best_weights(RGB_WEIGHTS_DIR, 'exp_a1_rgb_v2')\n",
    "\n",
    "# Find Depth weights (A.2)\n",
    "depth_weights = find_best_weights(DEPTH_WEIGHTS_DIR, 'exp_a2_depth')\n",
    "if depth_weights is None:\n",
    "    depth_weights = find_best_weights(DEPTH_WEIGHTS_DIR, 'exp_a2_depth_v2')\n",
    "\n",
    "print(\"Pre-trained Weights:\")\n",
    "print(f\"  RGB (A.1):   {rgb_weights}\")\n",
    "print(f\"  Depth (A.2): {depth_weights}\")\n",
    "\n",
    "# Check if weights exist\n",
    "if rgb_weights is None or depth_weights is None:\n",
    "    print(\"\\nWARNING: Pre-trained weights not found!\")\n",
    "    print(\"Please ensure A.1 and A.2 experiments have been run first.\")\n",
    "    print(\"\\nExpected paths:\")\n",
    "    print(f\"  RGB: {RGB_WEIGHTS_DIR}/exp_a1_rgb_seed42/weights/best.pt\")\n",
    "    print(f\"  Depth: {DEPTH_WEIGHTS_DIR}/exp_a2_depth_seed42/weights/best.pt\")\n",
    "    \n",
    "    # Fallback: use pretrained yolo11n.pt\n",
    "    print(\"\\nFallback: Using yolo11n.pt for both backbones\")\n",
    "    rgb_weights = 'yolo11n.pt'\n",
    "    depth_weights = 'yolo11n.pt'\n",
    "else:\n",
    "    print(\"\\nWeights found successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 9: Training Loop (5 Seeds) with Proper YOLO Loss\n# =============================================================================\n\nresults_all = {}\ntraining_times = {}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING LOOP - A.5 LATE FUSION\")\nprint(\"=\"*60)\nprint(\"\\nKey Features:\")\nprint(\"  - Multi-scale feature fusion (P3, P4, P5)\")\nprint(\"  - Proper YOLO v8DetectionLoss (box + cls + dfl)\")\nprint(\"  - Frozen RGB and Depth backbones\")\nprint(\"  - Trainable fusion layers and detection head\")\nprint(\"=\"*60)\n\nfor idx, seed in enumerate(SEEDS, 1):\n    start_time = time.time()\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"TRAINING A.5 LATE FUSION - Seed {seed} ({idx}/{len(SEEDS)})\")\n    print(f\"{'='*60}\")\n    \n    # Set random seeds\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    \n    try:\n        # Create datasets\n        train_dataset = LateFusionDataset(\n            rgb_img_dir=RGB_DATASET / 'images' / 'train',\n            depth_img_dir=DEPTH_DATASET / 'images' / 'train',\n            label_dir=RGB_DATASET / 'labels' / 'train',\n            img_size=IMGSZ,\n            augment=True,\n            augment_params=AUGMENT_PARAMS\n        )\n        \n        val_dataset = LateFusionDataset(\n            rgb_img_dir=RGB_DATASET / 'images' / 'val',\n            depth_img_dir=DEPTH_DATASET / 'images' / 'val',\n            label_dir=RGB_DATASET / 'labels' / 'val',\n            img_size=IMGSZ,\n            augment=False\n        )\n        \n        # Create dataloaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=True,\n            num_workers=NUM_WORKERS,\n            collate_fn=collate_fn,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False,\n            num_workers=NUM_WORKERS,\n            collate_fn=collate_fn,\n            pin_memory=True\n        )\n        \n        print(f\"\\nDataset loaded:\")\n        print(f\"  Train samples: {len(train_dataset)}\")\n        print(f\"  Val samples: {len(val_dataset)}\")\n        \n        # Create model\n        model = LateFusionModel(\n            rgb_model_path=rgb_weights,\n            depth_model_path=depth_weights,\n            num_classes=1,\n            device=DEVICE\n        )\n        \n        # Create save directory\n        save_dir = RUNS_PATH / f\"{EXP_PREFIX}_seed{seed}\"\n        \n        # Create trainer with proper YOLO loss\n        trainer = LateFusionTrainer(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            device=DEVICE,\n            epochs=EPOCHS,\n            patience=PATIENCE,\n            lr=0.01,\n            save_dir=save_dir,\n            num_classes=1,\n        )\n        \n        # Train\n        history = trainer.train()\n        \n        elapsed = time.time() - start_time\n        training_times[seed] = elapsed\n        \n        results_all[seed] = {\n            'model_path': str(save_dir / 'weights' / 'best.pt'),\n            'history': history,\n            'completed': True,\n            'best_fitness': trainer.best_fitness,\n        }\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Seed {seed} completed successfully!\")\n        print(f\"Best fitness: {trainer.best_fitness:.4f}\")\n        print(f\"Training time: {elapsed/60:.1f} minutes\")\n        print(f\"{'='*60}\")\n        \n    except Exception as e:\n        print(f\"\\n{'='*60}\")\n        print(f\"Seed {seed} failed: {e}\")\n        print(f\"{'='*60}\")\n        import traceback\n        traceback.print_exc()\n        results_all[seed] = {'error': str(e), 'completed': False}\n    \n    finally:\n        # Cleanup\n        if 'model' in locals():\n            del model\n        if 'trainer' in locals():\n            del trainer\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING LOOP COMPLETED\")\nprint(f\"Successful: {sum(1 for r in results_all.values() if r.get('completed', False))}/{len(SEEDS)}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 10: mAP Evaluation Functions (Proper Implementation)\n# =============================================================================\n\ndef box_iou(box1, box2):\n    \"\"\"\n    Compute IoU between two sets of boxes.\n    box1: [N, 4] in xyxy format\n    box2: [M, 4] in xyxy format\n    Returns: [N, M] IoU matrix\n    \"\"\"\n    def box_area(box):\n        return (box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1])\n\n    area1 = box_area(box1)\n    area2 = box_area(box2)\n\n    # Intersection\n    lt = torch.max(box1[:, None, :2], box2[None, :, :2])\n    rb = torch.min(box1[:, None, 2:], box2[None, :, 2:])\n    wh = (rb - lt).clamp(min=0)\n    inter = wh[:, :, 0] * wh[:, :, 1]\n\n    # Union\n    union = area1[:, None] + area2[None, :] - inter\n\n    return inter / (union + 1e-6)\n\n\ndef xywh_to_xyxy(boxes, img_size=640):\n    \"\"\"Convert boxes from xywh normalized to xyxy pixel format.\"\"\"\n    if len(boxes) == 0:\n        return torch.zeros((0, 4))\n\n    boxes = torch.tensor(boxes) if not isinstance(boxes, torch.Tensor) else boxes\n    x_center = boxes[:, 0] * img_size\n    y_center = boxes[:, 1] * img_size\n    w = boxes[:, 2] * img_size\n    h = boxes[:, 3] * img_size\n\n    x1 = x_center - w / 2\n    y1 = y_center - h / 2\n    x2 = x_center + w / 2\n    y2 = y_center + h / 2\n\n    return torch.stack([x1, y1, x2, y2], dim=1)\n\n\ndef compute_ap(recalls, precisions):\n    \"\"\"Compute Average Precision using 101-point interpolation (COCO style).\"\"\"\n    recalls = np.concatenate([[0.0], recalls, [1.0]])\n    precisions = np.concatenate([[1.0], precisions, [0.0]])\n\n    # Ensure precision is monotonically decreasing\n    for i in range(len(precisions) - 2, -1, -1):\n        precisions[i] = max(precisions[i], precisions[i + 1])\n\n    # 101-point interpolation\n    recall_levels = np.linspace(0, 1, 101)\n    ap = 0.0\n    for r in recall_levels:\n        prec_at_r = precisions[recalls >= r]\n        if len(prec_at_r) > 0:\n            ap += prec_at_r.max()\n\n    return ap / 101\n\n\ndef evaluate_detections(all_predictions, all_targets, iou_threshold=0.5, img_size=640):\n    \"\"\"\n    Evaluate detections and compute Precision, Recall, and AP.\n\n    Args:\n        all_predictions: List of (boxes, scores) per image, boxes in xywh normalized\n        all_targets: List of target boxes per image, in xywh normalized\n        iou_threshold: IoU threshold for matching\n\n    Returns:\n        dict with Precision, Recall, AP\n    \"\"\"\n    all_scores = []\n    all_matches = []  # 1 if TP, 0 if FP\n    total_gt = 0\n\n    for preds, targets in zip(all_predictions, all_targets):\n        pred_boxes, pred_scores = preds\n\n        if len(pred_boxes) == 0:\n            total_gt += len(targets)\n            continue\n\n        # Convert to xyxy\n        pred_xyxy = xywh_to_xyxy(pred_boxes, img_size)\n\n        if len(targets) == 0:\n            # All predictions are FP\n            for score in pred_scores:\n                all_scores.append(score)\n                all_matches.append(0)\n            continue\n\n        target_xyxy = xywh_to_xyxy(targets, img_size)\n        total_gt += len(targets)\n\n        # Compute IoU matrix\n        ious = box_iou(pred_xyxy, target_xyxy)\n\n        # Match predictions to targets (greedy matching)\n        matched_gt = set()\n\n        # Sort predictions by score (descending)\n        sorted_indices = np.argsort(pred_scores)[::-1]\n\n        for idx in sorted_indices:\n            score = pred_scores[idx]\n            all_scores.append(score)\n\n            if len(matched_gt) == len(targets):\n                all_matches.append(0)\n                continue\n\n            # Find best matching GT\n            iou_row = ious[idx].cpu().numpy()\n            best_gt_idx = -1\n            best_iou = iou_threshold\n\n            for gt_idx in range(len(targets)):\n                if gt_idx in matched_gt:\n                    continue\n                if iou_row[gt_idx] > best_iou:\n                    best_iou = iou_row[gt_idx]\n                    best_gt_idx = gt_idx\n\n            if best_gt_idx >= 0:\n                all_matches.append(1)  # TP\n                matched_gt.add(best_gt_idx)\n            else:\n                all_matches.append(0)  # FP\n\n    if len(all_scores) == 0 or total_gt == 0:\n        return {'Precision': 0.0, 'Recall': 0.0, 'AP': 0.0}\n\n    # Sort by score\n    sorted_indices = np.argsort(all_scores)[::-1]\n    all_matches = np.array(all_matches)[sorted_indices]\n\n    # Compute cumulative TP and FP\n    tp_cumsum = np.cumsum(all_matches)\n    fp_cumsum = np.cumsum(1 - all_matches)\n\n    # Precision and Recall at each threshold\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n    recalls = tp_cumsum / (total_gt + 1e-6)\n\n    # Final precision and recall\n    final_precision = float(precisions[-1]) if len(precisions) > 0 else 0.0\n    final_recall = float(recalls[-1]) if len(recalls) > 0 else 0.0\n\n    # Compute AP\n    ap = compute_ap(recalls, precisions)\n\n    return {\n        'Precision': final_precision,\n        'Recall': final_recall,\n        'AP': float(ap)\n    }\n\n\ndef nms_numpy(boxes, scores, iou_threshold=0.45):\n    \"\"\"\n    Simple NMS implementation.\n    boxes: [N, 4] in xyxy format (numpy)\n    scores: [N] (numpy)\n    Returns: indices to keep\n    \"\"\"\n    if len(boxes) == 0:\n        return np.array([], dtype=int)\n\n    boxes = np.array(boxes)\n    scores = np.array(scores)\n\n    # Sort by score\n    order = scores.argsort()[::-1]\n    keep = []\n\n    while len(order) > 0:\n        i = order[0]\n        keep.append(i)\n\n        if len(order) == 1:\n            break\n\n        # Compute IoU with rest\n        xx1 = np.maximum(boxes[i, 0], boxes[order[1:], 0])\n        yy1 = np.maximum(boxes[i, 1], boxes[order[1:], 1])\n        xx2 = np.minimum(boxes[i, 2], boxes[order[1:], 2])\n        yy2 = np.minimum(boxes[i, 3], boxes[order[1:], 3])\n\n        w = np.maximum(0, xx2 - xx1)\n        h = np.maximum(0, yy2 - yy1)\n        inter = w * h\n\n        area_i = (boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1])\n        area_rest = (boxes[order[1:], 2] - boxes[order[1:], 0]) * (boxes[order[1:], 3] - boxes[order[1:], 1])\n\n        iou = inter / (area_i + area_rest - inter + 1e-6)\n\n        # Keep boxes with IoU < threshold\n        inds = np.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n\n    return np.array(keep)\n\n\ndef decode_yolo_output(output, conf_threshold=0.25, iou_threshold=0.45, img_size=640):\n    \"\"\"\n    Decode YOLO output to boxes and scores.\n    \n    Args:\n        output: Raw model output tensor\n        conf_threshold: Confidence threshold\n        iou_threshold: NMS IoU threshold\n        img_size: Image size\n        \n    Returns:\n        boxes: [N, 4] in xywh normalized format\n        scores: [N] confidence scores\n    \"\"\"\n    # Handle different output formats\n    if isinstance(output, (list, tuple)):\n        output = output[0]\n    \n    # Expected shape: [batch, num_classes + 4, num_anchors] or [batch, num_anchors, num_classes + 4]\n    if output.dim() == 3:\n        if output.shape[1] == 5:  # [batch, 5, anchors] -> 1 class + 4 coords\n            output = output.permute(0, 2, 1)  # [batch, anchors, 5]\n        elif output.shape[2] == 5:  # Already [batch, anchors, 5]\n            pass\n        else:\n            # Try to reshape\n            output = output.view(output.shape[0], -1, 5)\n    \n    all_boxes = []\n    all_scores = []\n    \n    for batch_idx in range(output.shape[0]):\n        pred = output[batch_idx]  # [anchors, 5] or similar\n        \n        if pred.dim() == 1:\n            pred = pred.unsqueeze(0)\n        \n        # Assume format: x, y, w, h, conf (or conf first)\n        if pred.shape[-1] >= 5:\n            # Try conf at last position\n            conf = pred[:, 4] if pred.shape[-1] >= 5 else pred[:, 0]\n            boxes = pred[:, :4]\n        else:\n            continue\n        \n        # Filter by confidence\n        mask = conf > conf_threshold\n        conf = conf[mask]\n        boxes = boxes[mask]\n        \n        if len(boxes) == 0:\n            all_boxes.append(np.zeros((0, 4)))\n            all_scores.append(np.array([]))\n            continue\n        \n        # Convert to numpy\n        boxes_np = boxes.cpu().numpy()\n        scores_np = conf.cpu().numpy()\n        \n        # Ensure boxes are in valid range [0, 1]\n        boxes_np = np.clip(boxes_np, 0, 1)\n        \n        # Convert to xyxy for NMS\n        x_center = boxes_np[:, 0] * img_size\n        y_center = boxes_np[:, 1] * img_size\n        w = boxes_np[:, 2] * img_size\n        h = boxes_np[:, 3] * img_size\n        \n        x1 = x_center - w / 2\n        y1 = y_center - h / 2\n        x2 = x_center + w / 2\n        y2 = y_center + h / 2\n        \n        boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)\n        \n        # Apply NMS\n        keep = nms_numpy(boxes_xyxy, scores_np, iou_threshold)\n        \n        if len(keep) > 0:\n            all_boxes.append(boxes_np[keep])\n            all_scores.append(scores_np[keep])\n        else:\n            all_boxes.append(np.zeros((0, 4)))\n            all_scores.append(np.array([]))\n    \n    return all_boxes, all_scores\n\nprint(\"mAP evaluation functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 11: Evaluation on Test Set (Proper mAP)\n# =============================================================================\n\nresults_dict = {}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION ON TEST SET (mAP)\")\nprint(\"=\"*60)\n\n# Create test dataset\ntest_dataset = LateFusionDataset(\n    rgb_img_dir=RGB_DATASET / 'images' / 'test',\n    depth_img_dir=DEPTH_DATASET / 'images' / 'test',\n    label_dir=RGB_DATASET / 'labels' / 'test',\n    img_size=IMGSZ,\n    augment=False\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    collate_fn=collate_fn\n)\n\nprint(f\"Test samples: {len(test_dataset)}\")\n\nfor seed in SEEDS:\n    model_path = RUNS_PATH / f\"{EXP_PREFIX}_seed{seed}\" / 'weights' / 'best.pt'\n    \n    if not model_path.exists():\n        print(f\"\\nSeed {seed}: Model not found at {model_path}\")\n        continue\n    \n    print(f\"\\nEvaluating Seed {seed}...\")\n    \n    try:\n        # Load model\n        model = LateFusionModel(\n            rgb_model_path=rgb_weights,\n            depth_model_path=depth_weights,\n            device=DEVICE\n        )\n        model = model.to(DEVICE)\n        \n        # Load trained weights\n        checkpoint = torch.load(model_path, map_location=DEVICE)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.eval()\n        \n        # Collect predictions and targets\n        all_predictions = []\n        all_targets = []\n        \n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=f\"Seed {seed}\"):\n                rgb = batch['rgb'].to(DEVICE)\n                depth = batch['depth'].to(DEVICE)\n                labels = batch['labels']  # [batch_idx, class, x, y, w, h]\n                \n                # Forward pass\n                outputs = model(rgb, depth)\n                \n                # Decode predictions\n                pred_boxes, pred_scores = decode_yolo_output(\n                    outputs, \n                    conf_threshold=0.25, \n                    iou_threshold=0.45,\n                    img_size=IMGSZ\n                )\n                \n                # Get batch size\n                batch_size = rgb.shape[0]\n                \n                for b in range(batch_size):\n                    # Get predictions for this image\n                    if b < len(pred_boxes):\n                        boxes = pred_boxes[b]\n                        scores = pred_scores[b]\n                    else:\n                        boxes = np.zeros((0, 4))\n                        scores = np.array([])\n                    \n                    all_predictions.append((boxes, scores))\n                    \n                    # Get targets for this image\n                    if len(labels) > 0:\n                        img_labels = labels[labels[:, 0] == b]\n                        if len(img_labels) > 0:\n                            # Extract boxes: [class, x, y, w, h] -> [x, y, w, h]\n                            target_boxes = img_labels[:, 2:6].numpy()\n                        else:\n                            target_boxes = np.zeros((0, 4))\n                    else:\n                        target_boxes = np.zeros((0, 4))\n                    \n                    all_targets.append(target_boxes)\n        \n        # Compute metrics at IoU 0.5\n        metrics_50 = evaluate_detections(all_predictions, all_targets, iou_threshold=0.5, img_size=IMGSZ)\n        \n        # Compute mAP50-95 (average over IoU thresholds 0.5 to 0.95)\n        ap_values = []\n        for iou_thresh in np.arange(0.5, 0.96, 0.05):\n            metrics = evaluate_detections(all_predictions, all_targets, iou_threshold=iou_thresh, img_size=IMGSZ)\n            ap_values.append(metrics['AP'])\n        \n        map50_95 = np.mean(ap_values)\n        \n        results_dict[seed] = {\n            'mAP50': metrics_50['AP'],\n            'mAP50-95': map50_95,\n            'Precision': metrics_50['Precision'],\n            'Recall': metrics_50['Recall'],\n        }\n        \n        print(f\"  mAP50:     {metrics_50['AP']:.4f}\")\n        print(f\"  mAP50-95:  {map50_95:.4f}\")\n        print(f\"  Precision: {metrics_50['Precision']:.4f}\")\n        print(f\"  Recall:    {metrics_50['Recall']:.4f}\")\n        \n        del model\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"  Evaluation failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION COMPLETED\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 12: Results Summary (Same Format as A.1-A.4b)\n# =============================================================================\n\nif results_dict:\n    df = pd.DataFrame(results_dict).T\n    df.index.name = 'Seed'\n    \n    # Calculate statistics\n    avg = df.mean()\n    std = df.std()\n    min_vals = df.min()\n    max_vals = df.max()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"A.5 LATE FUSION (V2) - FINAL RESULTS\")\n    print(\"=\"*60 + \"\\n\")\n    \n    print(\"Per-Seed Results:\")\n    print(df.to_string(float_format=lambda x: f\"{x:.4f}\"))\n    \n    print(\"\\n\" + \"-\"*60)\n    print(\"STATISTICAL SUMMARY\")\n    print(\"-\"*60)\n    print(f\"{'Metric':<15} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n    print(\"-\"*60)\n    for col in df.columns:\n        print(f\"{col:<15} {avg[col]:>10.4f} {std[col]:>10.4f} {min_vals[col]:>10.4f} {max_vals[col]:>10.4f}\")\n    \n    # Best seed\n    best_seed = df['mAP50'].idxmax()\n    print(f\"\\nBest Seed: {best_seed} (mAP50: {df.loc[best_seed, 'mAP50']:.4f})\")\n    \n    print(\"=\"*60)\nelse:\n    print(\"No results to display. Training may have failed.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Cell 13: Save Results (Same Format as A.1-A.4b)\n# =============================================================================\n\noutput_file = KAGGLE_OUTPUT / 'a5_late_fusion_v2_results.txt'\n\nwith open(output_file, 'w') as f:\n    f.write(\"=\"*60 + \"\\n\")\n    f.write(\"A.5 Late Fusion (V2) Results\\n\")\n    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    f.write(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    \n    f.write(\"Configuration:\\n\")\n    f.write(f\"  Architecture: Late Fusion (RGB + Depth)\\n\")\n    f.write(f\"  RGB Backbone: Frozen (from A.1)\\n\")\n    f.write(f\"  Depth Backbone: Frozen (from A.2)\\n\")\n    f.write(f\"  Trainable: Fusion Layer + Detection Head\\n\")\n    f.write(f\"  Model: YOLOv11n (backbones)\\n\")\n    f.write(f\"  Epochs: {EPOCHS} (patience: {PATIENCE})\\n\")\n    f.write(f\"  Image Size: {IMGSZ}\\n\")\n    f.write(f\"  Batch Size: {BATCH_SIZE}\\n\")\n    f.write(f\"  Seeds: {SEEDS}\\n\")\n    \n    f.write(\"\\nUniform Augmentation:\\n\")\n    for key, value in AUGMENT_PARAMS.items():\n        f.write(f\"  {key}: {value}\\n\")\n    \n    if results_dict:\n        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n        f.write(\"Per-Seed Results:\\n\")\n        f.write(\"=\"*60 + \"\\n\")\n        f.write(df.to_string(float_format=lambda x: f\"{x:.4f}\"))\n        \n        f.write(\"\\n\\n\" + \"-\"*60 + \"\\n\")\n        f.write(\"Summary (Mean +/- Std):\\n\")\n        f.write(\"-\"*60 + \"\\n\")\n        for col in df.columns:\n            f.write(f\"  {col}: {avg[col]:.4f} +/- {std[col]:.4f}\\n\")\n        \n        f.write(f\"\\nBest Seed: {best_seed}\\n\")\n\nprint(f\"\\nResults saved: {output_file}\")\n\n# Save as JSON (same format as other experiments)\njson_output = {\n    'experiment': 'A.5',\n    'variant': 'V2',\n    'name': 'Late Fusion',\n    'seeds': SEEDS,\n    'config': {\n        'model': 'yolo11n',\n        'architecture': 'late_fusion',\n        'rgb_backbone': 'frozen_from_a1',\n        'depth_backbone': 'frozen_from_a2',\n        'epochs': EPOCHS,\n        'patience': PATIENCE,\n        'imgsz': IMGSZ,\n        'batch': BATCH_SIZE,\n        'augmentation': AUGMENT_PARAMS,\n    },\n    'results': {str(k): v for k, v in results_dict.items()} if results_dict else {},\n    'summary': {\n        'mean': {k: float(v) for k, v in avg.items()},\n        'std': {k: float(v) for k, v in std.items()},\n        'best_seed': int(best_seed) if results_dict else None,\n    } if results_dict else None,\n}\n\njson_file = KAGGLE_OUTPUT / 'a5_late_fusion_v2_results.json'\nwith open(json_file, 'w') as f:\n    json.dump(json_output, f, indent=2)\n\nprint(f\"JSON saved: {json_file}\")"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Cell 14: Create Archives for Download\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING ARCHIVES\")\nprint(\"=\"*60 + \"\\n\")\n\n# Archive training runs\nif RUNS_PATH.exists():\n    runs_zip = BASE_PATH / 'a5_late_fusion_v2_runs'\n    shutil.make_archive(str(runs_zip), 'zip', RUNS_PATH)\n    size_mb = (runs_zip.with_suffix('.zip')).stat().st_size / 1024 / 1024\n    print(f\"a5_late_fusion_v2_runs.zip: {size_mb:.1f} MB\")\n\n# Archive outputs\noutput_zip = BASE_PATH / 'a5_late_fusion_v2_output'\nshutil.make_archive(str(output_zip), 'zip', KAGGLE_OUTPUT)\nsize_mb = (output_zip.with_suffix('.zip')).stat().st_size / 1024 / 1024\nprint(f\"a5_late_fusion_v2_output.zip: {size_mb:.1f} MB\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALL DONE!\")\nprint(\"=\"*60)\nprint(\"\\nDownload from Output tab:\")\nprint(\"  - a5_late_fusion_v2_runs.zip (training runs)\")\nprint(\"  - a5_late_fusion_v2_output.zip (results)\")\nprint(\"\\nOutput files:\")\nprint(f\"  - {output_file}\")\nprint(f\"  - {json_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}