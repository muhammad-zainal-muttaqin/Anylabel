{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14537315,"sourceType":"datasetVersion","datasetId":9284917},{"sourceId":14549067,"sourceType":"datasetVersion","datasetId":9286041},{"sourceId":734764,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":560056,"modelId":572643},{"sourceId":734765,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":560057,"modelId":572644}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A.5 Late Fusion (Frozen RGB + Frozen Depth + Trainable Fusion)\n\n**Experiment:** A.5  \n**Architecture:** Late Fusion (Two-Stream with Feature Fusion)  \n**Input:** RGB (3-channel) + Depth (3-channel) - processed separately  \n**Objective:** Combine RGB and Depth features for improved detection  \n**Classes:** 1 (fresh_fruit_bunch)\n\n## Architecture Overview\n\n```\nRGB Image (3ch)          Depth Image (3ch)\n     |                        |\n     v                        v\n[Frozen RGB Backbone]    [Frozen Depth Backbone]\n  (from A.1 weights)       (from A.2 weights)\n     |                        |\n     +-----> P3 Features <----+\n              (256 ch each)\n                   |\n                   v\n         [Concatenate: 512 ch]\n                   |\n                   v\n         [1x1 Conv: 512 -> 256]\n         [BatchNorm + SiLU]\n                   |\n                   v\n         [YOLO Detection Head]\n              (trainable)\n                   |\n              [Output]\n```\n\n## Key Features\n- **Frozen Backbones:** RGB (A.1) and Depth (A.2) backbones are 100% frozen\n- **Trainable Components:** Only fusion layer (1x1 Conv) and detection head\n- **Dual Input:** Separate RGB and Depth images loaded together\n- **Memory Note:** Batch size 8 due to dual backbone forward pass\n\n## Uniform Augmentation (All Experiments)\n- translate: 0.1\n- scale: 0.5\n- fliplr: 0.5\n- hsv_h: 0.0 (disabled)\n- hsv_s: 0.0 (disabled)\n- hsv_v: 0.0 (disabled)\n- erasing: 0.0\n- mosaic: 0.0\n- mixup: 0.0","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# Cell 1: Environment Setup & Install\n# =============================================================================\nimport os\nimport sys\nfrom pathlib import Path\n\n# Detect environment\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\nif IS_KAGGLE:\n    BASE_PATH = Path('/kaggle/working')\nelse:\n    BASE_PATH = Path(r'D:/Work/Assisten Dosen/Anylabel/Experiments')\n\n# Install dependencies\n!pip install -q ultralytics\n\nprint(\"=\"*60)\nprint(\"A.5 LATE FUSION - ENVIRONMENT SETUP\")\nprint(\"=\"*60)\nprint(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Local'}\")\nprint(f\"Base Path: {BASE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:04:54.072304Z","iopub.execute_input":"2026-01-29T02:04:54.072601Z","iopub.status.idle":"2026-01-29T02:04:59.982872Z","shell.execute_reply.started":"2026-01-29T02:04:54.072568Z","shell.execute_reply":"2026-01-29T02:04:59.982073Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h============================================================\nA.5 LATE FUSION - ENVIRONMENT SETUP\n============================================================\nRunning on: Kaggle\nBase Path: /kaggle/working\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# Cell 2: Imports\n# =============================================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport json\nimport gc\nimport time\nimport random\nfrom datetime import datetime\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple, Optional\nfrom copy import deepcopy\n\nfrom ultralytics import YOLO\nfrom ultralytics.nn.tasks import DetectionModel\nfrom ultralytics.utils.loss import v8DetectionLoss\nfrom ultralytics.utils.ops import xywh2xyxy\n\n# Disable wandb logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:04:59.984983Z","iopub.execute_input":"2026-01-29T02:04:59.985314Z","iopub.status.idle":"2026-01-29T02:05:07.716407Z","shell.execute_reply.started":"2026-01-29T02:04:59.985281Z","shell.execute_reply":"2026-01-29T02:05:07.715675Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nPyTorch: 2.8.0+cu126\nCUDA Available: True\nGPU: Tesla T4\nCUDA Memory: 15.8 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# Cell 3: Configuration - UNIFORM AUGMENTATION\n# =============================================================================\n\n# Uniform augmentation parameters (MUST match A.1-A.4b)\nAUGMENT_PARAMS = {\n    'translate': 0.1,\n    'scale': 0.5,\n    'fliplr': 0.5,\n    'hsv_h': 0.0,      # Disabled for uniformity\n    'hsv_s': 0.0,      # Disabled for uniformity\n    'hsv_v': 0.0,      # Disabled for uniformity\n    'erasing': 0.0,\n    'mosaic': 0.0,     # Disabled for uniformity\n    'mixup': 0.0,\n    'degrees': 0.0,\n    'copy_paste': 0.0,\n}\n\n# Training parameters\nSEEDS = [42, 123, 456, 789, 101]\nEXP_PREFIX = \"exp_a5_fusion\"\nEPOCHS = 5\nPATIENCE = 30\nIMGSZ = 640\nBATCH_SIZE = 16 \nDEVICE = 0 if torch.cuda.is_available() else 'cpu'\nNUM_WORKERS = 4 if not IS_KAGGLE else 2\n\nprint(\"=\"*60)\nprint(\"A.5 LATE FUSION - TRAINING CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"Experiment:   A.5 Late Fusion\")\nprint(f\"Seeds:        {SEEDS} ({len(SEEDS)} runs)\")\nprint(f\"Epochs:       {EPOCHS} (patience: {PATIENCE})\")\nprint(f\"Image Size:   {IMGSZ}\")\nprint(f\"Batch Size:   {BATCH_SIZE} (reduced for dual backbone)\")\nprint(f\"Device:       {DEVICE}\")\nprint(f\"\\nArchitecture:\")\nprint(f\"  RGB Backbone:   FROZEN (from A.1)\")\nprint(f\"  Depth Backbone: FROZEN (from A.2)\")\nprint(f\"  Trainable:      Fusion Layer + Detection Head\")\nprint(f\"\\nUniform Augmentation:\")\nfor k, v in AUGMENT_PARAMS.items():\n    print(f\"  {k}: {v}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.717250Z","iopub.execute_input":"2026-01-29T02:05:07.717583Z","iopub.status.idle":"2026-01-29T02:05:07.725348Z","shell.execute_reply.started":"2026-01-29T02:05:07.717559Z","shell.execute_reply":"2026-01-29T02:05:07.724665Z"}},"outputs":[{"name":"stdout","text":"============================================================\nA.5 LATE FUSION - TRAINING CONFIGURATION\n============================================================\nExperiment:   A.5 Late Fusion\nSeeds:        [42, 123, 456, 789, 101] (5 runs)\nEpochs:       5 (patience: 30)\nImage Size:   640\nBatch Size:   16 (reduced for dual backbone)\nDevice:       0\n\nArchitecture:\n  RGB Backbone:   FROZEN (from A.1)\n  Depth Backbone: FROZEN (from A.2)\n  Trainable:      Fusion Layer + Detection Head\n\nUniform Augmentation:\n  translate: 0.1\n  scale: 0.5\n  fliplr: 0.5\n  hsv_h: 0.0\n  hsv_s: 0.0\n  hsv_v: 0.0\n  erasing: 0.0\n  mosaic: 0.0\n  mixup: 0.0\n  degrees: 0.0\n  copy_paste: 0.0\n============================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# Cell 4: Paths Configuration\n# =============================================================================\n\nif IS_KAGGLE:\n    # Kaggle paths - adjust dataset names as needed\n    RGB_DATASET = Path('/kaggle/input/ffb-localization-dataset/ffb_localization')\n    DEPTH_DATASET = Path('/kaggle/input/ffb-localization-depth-dataset/ffb_localization_depth')\n    # Pre-trained weights from A.1 and A.2 (upload as datasets)\n    RGB_WEIGHTS_DIR = Path('/kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1')\n    DEPTH_WEIGHTS_DIR = Path('/kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1')\nelse:\n    # Local paths\n    RGB_DATASET = BASE_PATH / 'datasets' / 'ffb_localization'\n    DEPTH_DATASET = BASE_PATH / 'datasets' / 'ffb_localization_depth'\n    RGB_WEIGHTS_DIR = BASE_PATH / 'runs' / 'detect'\n    DEPTH_WEIGHTS_DIR = BASE_PATH / 'runs' / 'detect'\n\nRUNS_PATH = BASE_PATH / 'runs' / 'detect'\nKAGGLE_OUTPUT = BASE_PATH / 'kaggleoutput'\nRUNS_PATH.mkdir(parents=True, exist_ok=True)\nKAGGLE_OUTPUT.mkdir(parents=True, exist_ok=True)\n\nprint(\"Paths Configuration:\")\nprint(f\"  RGB Dataset:     {RGB_DATASET}\")\nprint(f\"  Depth Dataset:   {DEPTH_DATASET}\")\nprint(f\"  RGB Weights Dir: {RGB_WEIGHTS_DIR}\")\nprint(f\"  Depth Weights:   {DEPTH_WEIGHTS_DIR}\")\nprint(f\"  Runs Path:       {RUNS_PATH}\")\nprint(f\"  Output Path:     {KAGGLE_OUTPUT}\")\n\n# Verify datasets exist\nprint(f\"\\nDataset Verification:\")\nfor name, path in [('RGB', RGB_DATASET), ('Depth', DEPTH_DATASET)]:\n    exists = path.exists()\n    print(f\"  {name}: {'OK' if exists else 'NOT FOUND'} - {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.726280Z","iopub.execute_input":"2026-01-29T02:05:07.726741Z","iopub.status.idle":"2026-01-29T02:05:07.760410Z","shell.execute_reply.started":"2026-01-29T02:05:07.726684Z","shell.execute_reply":"2026-01-29T02:05:07.759656Z"}},"outputs":[{"name":"stdout","text":"Paths Configuration:\n  RGB Dataset:     /kaggle/input/ffb-localization-dataset/ffb_localization\n  Depth Dataset:   /kaggle/input/ffb-localization-depth-dataset/ffb_localization_depth\n  RGB Weights Dir: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1\n  Depth Weights:   /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1\n  Runs Path:       /kaggle/working/runs/detect\n  Output Path:     /kaggle/working/kaggleoutput\n\nDataset Verification:\n  RGB: OK - /kaggle/input/ffb-localization-dataset/ffb_localization\n  Depth: OK - /kaggle/input/ffb-localization-depth-dataset/ffb_localization_depth\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =============================================================================\n# Cell 5: Dual-Input Dataset Class\n# =============================================================================\n\nclass LateFusionDataset(Dataset):\n    \"\"\"\n    Dataset that loads RGB and Depth images separately for late fusion.\n    Applies synchronized geometric augmentation to both modalities.\n    \"\"\"\n    \n    def __init__(\n        self,\n        rgb_img_dir: Path,\n        depth_img_dir: Path,\n        label_dir: Path,\n        img_size: int = 640,\n        augment: bool = False,\n        augment_params: dict = None\n    ):\n        self.rgb_img_dir = Path(rgb_img_dir)\n        self.depth_img_dir = Path(depth_img_dir)\n        self.label_dir = Path(label_dir)\n        self.img_size = img_size\n        self.augment = augment\n        self.augment_params = augment_params or {}\n        \n        # Get list of files (use RGB as reference)\n        self.image_files = sorted([p.name for p in self.rgb_img_dir.glob('*.png')])\n        \n        # Filter to only include files that exist in both RGB and Depth\n        valid_files = []\n        for fname in self.image_files:\n            rgb_exists = (self.rgb_img_dir / fname).exists()\n            depth_exists = (self.depth_img_dir / fname).exists()\n            label_exists = (self.label_dir / fname.replace('.png', '.txt')).exists()\n            if rgb_exists and depth_exists and label_exists:\n                valid_files.append(fname)\n        \n        self.image_files = valid_files\n        print(f\"[LateFusionDataset] Loaded {len(self)} valid samples\")\n    \n    def __len__(self) -> int:\n        return len(self.image_files)\n    \n    def _apply_augmentation(self, rgb, depth, labels):\n        \"\"\"\n        Apply synchronized geometric augmentation to RGB and Depth.\n        Only geometric transforms (translate, scale, fliplr) are applied.\n        \"\"\"\n        h, w = rgb.shape[:2]\n        \n        # Horizontal flip\n        if random.random() < self.augment_params.get('fliplr', 0.0):\n            rgb = cv2.flip(rgb, 1)\n            depth = cv2.flip(depth, 1)\n            if len(labels) > 0:\n                labels[:, 1] = 1.0 - labels[:, 1]  # Flip x_center\n        \n        # Scale and translate (affine transform)\n        scale = self.augment_params.get('scale', 0.0)\n        translate = self.augment_params.get('translate', 0.0)\n        \n        if scale > 0 or translate > 0:\n            # Random scale factor\n            s = random.uniform(1 - scale, 1 + scale)\n            \n            # Random translation\n            tx = random.uniform(-translate, translate) * w\n            ty = random.uniform(-translate, translate) * h\n            \n            # Affine matrix\n            M = np.array([\n                [s, 0, tx + (1 - s) * w / 2],\n                [0, s, ty + (1 - s) * h / 2]\n            ], dtype=np.float32)\n            \n            # Apply to both images\n            rgb = cv2.warpAffine(rgb, M, (w, h), borderValue=(114, 114, 114))\n            depth = cv2.warpAffine(depth, M, (w, h), borderValue=(0, 0, 0))\n            \n            # Transform labels\n            if len(labels) > 0:\n                # Convert to pixel coordinates\n                x_center = labels[:, 1] * w\n                y_center = labels[:, 2] * h\n                box_w = labels[:, 3] * w\n                box_h = labels[:, 4] * h\n                \n                # Apply transformation\n                x_center = x_center * s + tx + (1 - s) * w / 2\n                y_center = y_center * s + ty + (1 - s) * h / 2\n                box_w = box_w * s\n                box_h = box_h * s\n                \n                # Convert back to normalized\n                labels[:, 1] = x_center / w\n                labels[:, 2] = y_center / h\n                labels[:, 3] = box_w / w\n                labels[:, 4] = box_h / h\n                \n                # Clip to valid range\n                labels[:, 1:] = np.clip(labels[:, 1:], 0, 1)\n                \n                # Filter out invalid boxes\n                valid = (labels[:, 3] > 0.001) & (labels[:, 4] > 0.001)\n                labels = labels[valid]\n        \n        return rgb, depth, labels\n    \n    def __getitem__(self, idx: int) -> Dict:\n        \"\"\"\n        Get a single sample with RGB, Depth, and labels.\n        \"\"\"\n        fname = self.image_files[idx]\n        \n        # Load RGB (BGR format from cv2)\n        rgb_path = self.rgb_img_dir / fname\n        rgb = cv2.imread(str(rgb_path))\n        \n        # Load Depth (3-channel from processed depth)\n        depth_path = self.depth_img_dir / fname\n        depth = cv2.imread(str(depth_path))\n        \n        if depth is None:\n            # Fallback: load as grayscale and convert to 3-channel\n            depth = cv2.imread(str(depth_path), cv2.IMREAD_GRAYSCALE)\n            if depth is not None:\n                depth = cv2.cvtColor(depth, cv2.COLOR_GRAY2BGR)\n            else:\n                # Last fallback: use zeros\n                depth = np.zeros_like(rgb)\n        \n        # Load labels (YOLO format: class x_center y_center width height)\n        label_path = self.label_dir / fname.replace('.png', '.txt')\n        if label_path.exists():\n            labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n            if labels.size == 0:\n                labels = np.zeros((0, 5), dtype=np.float32)\n        else:\n            labels = np.zeros((0, 5), dtype=np.float32)\n        \n        # Resize if needed\n        if rgb.shape[:2] != (self.img_size, self.img_size):\n            rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n            depth = cv2.resize(depth, (self.img_size, self.img_size))\n        \n        # Apply augmentation (synchronized)\n        if self.augment:\n            rgb, depth, labels = self._apply_augmentation(rgb, depth, labels)\n        \n        # Convert BGR to RGB for model input\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        depth = cv2.cvtColor(depth, cv2.COLOR_BGR2RGB)\n        \n        # Normalize to [0, 1] and convert to tensor\n        rgb_tensor = torch.from_numpy(rgb).permute(2, 0, 1).float() / 255.0\n        depth_tensor = torch.from_numpy(depth).permute(2, 0, 1).float() / 255.0\n        \n        # Labels tensor\n        labels_tensor = torch.from_numpy(labels).float()\n        \n        return {\n            'rgb': rgb_tensor,\n            'depth': depth_tensor,\n            'labels': labels_tensor,\n            'batch_idx': torch.zeros(len(labels)),\n            'img_path': str(rgb_path),\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function for variable-length labels.\"\"\"\n    rgb = torch.stack([item['rgb'] for item in batch])\n    depth = torch.stack([item['depth'] for item in batch])\n    \n    # Handle labels with batch index\n    labels_list = []\n    for i, item in enumerate(batch):\n        labels = item['labels']\n        if len(labels) > 0:\n            batch_idx = torch.full((len(labels), 1), i, dtype=torch.float32)\n            labels_with_idx = torch.cat([batch_idx, labels], dim=1)\n            labels_list.append(labels_with_idx)\n    \n    if labels_list:\n        labels = torch.cat(labels_list, dim=0)\n    else:\n        labels = torch.zeros((0, 6), dtype=torch.float32)\n    \n    return {\n        'rgb': rgb,\n        'depth': depth,\n        'labels': labels,\n        'img_paths': [item['img_path'] for item in batch],\n    }\n\nprint(\"LateFusionDataset class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.761393Z","iopub.execute_input":"2026-01-29T02:05:07.761614Z","iopub.status.idle":"2026-01-29T02:05:07.782474Z","shell.execute_reply.started":"2026-01-29T02:05:07.761590Z","shell.execute_reply":"2026-01-29T02:05:07.781702Z"}},"outputs":[{"name":"stdout","text":"LateFusionDataset class defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =============================================================================\n# Cell 6: Late Fusion Model Architecture (DYNAMIC & ROBUST FIX)\n# =============================================================================\n\nclass LateFusionModel(nn.Module):\n    \"\"\"\n    Late Fusion Model for FFB Detection with Dynamic Channel Configuration.\n    \n    This model automatically detects the output channels of the backbone\n    and the input requirements of the detection head, then builds the \n    fusion layers to bridge them perfectly.\n    \"\"\"\n    \n    def __init__(\n        self,\n        rgb_model_path: str,\n        depth_model_path: str,\n        num_classes: int = 1,\n        device: str = 'cuda'\n    ):\n        super().__init__()\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"Initializing Late Fusion Model (Dynamic Configuration)\")\n        print(\"=\"*60)\n        \n        self.device = device\n        self.num_classes = num_classes\n        \n        # 1. Load RGB Backbone\n        print(f\"\\nLoading RGB backbone from: {rgb_model_path}\")\n        rgb_yolo_tmp = YOLO(rgb_model_path)\n        self.rgb_backbone = rgb_yolo_tmp.model.model \n        for param in self.rgb_backbone.parameters():\n            param.requires_grad = False\n        self.rgb_backbone.eval()\n        \n        # 2. Load Depth Backbone\n        print(f\"Loading Depth backbone from: {depth_model_path}\")\n        depth_yolo_tmp = YOLO(depth_model_path)\n        self.depth_backbone = depth_yolo_tmp.model.model\n        for param in self.depth_backbone.parameters():\n            param.requires_grad = False\n        self.depth_backbone.eval()\n        \n        # 3. Dynamic Shape Discovery\n        # We run a dummy input through the backbone to see exactly what comes out.\n        # This removes all guesswork about channel sizes.\n        print(\"\\nAnalyzing backbone output shapes...\")\n        dummy_input = torch.zeros(1, 3, 640, 640)\n        with torch.no_grad():\n            # Extract features from RGB backbone\n            dummy_features = self._extract_features(self.rgb_backbone, dummy_input)\n            \n        # Get channel counts from backbone outputs (P3, P4, P5)\n        # Note: We multiply by 2 later because we concat RGB + Depth\n        c3_backbone = dummy_features[0].shape[1]\n        c4_backbone = dummy_features[1].shape[1]\n        c5_backbone = dummy_features[2].shape[1]\n        \n        print(f\"  Backbone Output Channels: P3={c3_backbone}, P4={c4_backbone}, P5={c5_backbone}\")\n        \n        # 4. Analyze Detection Head Requirements\n        # The head expects specific input channels. We extract this from the head itself.\n        self.detect = deepcopy(rgb_yolo_tmp.model.model[-1])\n        \n        # In Ultralytics, head.ch contains the list of expected input channels [ch_p3, ch_p4, ch_p5]\n        if hasattr(self.detect, 'ch'):\n            head_reqs = self.detect.ch\n        else:\n            # Fallback for older versions: inspect the first conv layer of each scale\n            print(\"  WARNING: head.ch not found, inferring from cv2 layers...\")\n            head_reqs = [m[0].conv.in_channels for m in self.detect.cv2]\n\n        h3_req, h4_req, h5_req = head_reqs\n        print(f\"  Head Input Requirements:  P3={h3_req}, P4={h4_req}, P5={h5_req}\")\n        \n        # 5. Build Fusion Layers Dynamically\n        # Input = Backbone*2 (RGB+Depth), Output = Head Requirement\n        print(f\"\\nConfiguring Fusion Layers:\")\n        \n        # P3\n        in_p3 = c3_backbone * 2\n        print(f\"  Fusion P3: {in_p3} -> {h3_req}\")\n        self.fusion_p3 = nn.Sequential(\n            nn.Conv2d(in_p3, h3_req, kernel_size=1, bias=False),\n            nn.BatchNorm2d(h3_req),\n            nn.SiLU(inplace=True)\n        )\n        \n        # P4\n        in_p4 = c4_backbone * 2\n        print(f\"  Fusion P4: {in_p4} -> {h4_req}\")\n        self.fusion_p4 = nn.Sequential(\n            nn.Conv2d(in_p4, h4_req, kernel_size=1, bias=False),\n            nn.BatchNorm2d(h4_req),\n            nn.SiLU(inplace=True)\n        )\n        \n        # P5\n        in_p5 = c5_backbone * 2\n        print(f\"  Fusion P5: {in_p5} -> {h5_req}\")\n        self.fusion_p5 = nn.Sequential(\n            nn.Conv2d(in_p5, h5_req, kernel_size=1, bias=False),\n            nn.BatchNorm2d(h5_req),\n            nn.SiLU(inplace=True)\n        )\n        \n        # Initialize weights\n        for m in [self.fusion_p3, self.fusion_p4, self.fusion_p5]:\n            for layer in m.modules():\n                if isinstance(layer, nn.Conv2d):\n                    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n        \n        # Enable gradients for head\n        for param in self.detect.parameters():\n            param.requires_grad = True\n            \n        self._count_parameters()\n        \n        # Cleanup\n        del rgb_yolo_tmp\n        del depth_yolo_tmp\n        del dummy_input\n        del dummy_features\n    \n    def _count_parameters(self):\n        \"\"\"Count total and trainable parameters.\"\"\"\n        total = sum(p.numel() for p in self.parameters())\n        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        frozen = total - trainable\n        \n        print(f\"\\n[Parameter Count]\")\n        print(f\"  Total:      {total:,}\")\n        print(f\"  Trainable:  {trainable:,} ({100*trainable/total:.1f}%)\")\n        print(f\"  Frozen:     {frozen:,} ({100*frozen/total:.1f}%)\")\n        print(\"=\"*60)\n    \n    def _extract_features(self, backbone, x):\n        \"\"\"\n        Extract multi-scale features (P3, P4, P5) from backbone.\n        \"\"\"\n        features = []\n        extract_layers = [4, 6, 8]  # P3, P4, P5\n        \n        for i, module in enumerate(backbone):\n            x = module(x)\n            if i in extract_layers:\n                features.append(x)\n                if len(features) == len(extract_layers):\n                    break\n        \n        return features  # [P3, P4, P5]\n    \n    def forward(self, rgb: torch.Tensor, depth: torch.Tensor):\n        # Extract features\n        with torch.no_grad():\n            rgb_features = self._extract_features(self.rgb_backbone, rgb)\n            depth_features = self._extract_features(self.depth_backbone, depth)\n        \n        # Fuse\n        fused_p3 = self.fusion_p3(torch.cat([rgb_features[0], depth_features[0]], dim=1))\n        fused_p4 = self.fusion_p4(torch.cat([rgb_features[1], depth_features[1]], dim=1))\n        fused_p5 = self.fusion_p5(torch.cat([rgb_features[2], depth_features[2]], dim=1))\n        \n        # Detect\n        return self.detect([fused_p3, fused_p4, fused_p5])\n    \n    def train(self, mode=True):\n        super().train(mode)\n        self.rgb_backbone.eval()\n        self.depth_backbone.eval()\n        return self\n\nprint(\"LateFusionModel class defined with DYNAMIC configuration\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.783444Z","iopub.execute_input":"2026-01-29T02:05:07.783658Z","iopub.status.idle":"2026-01-29T02:05:07.802250Z","shell.execute_reply.started":"2026-01-29T02:05:07.783636Z","shell.execute_reply":"2026-01-29T02:05:07.801528Z"}},"outputs":[{"name":"stdout","text":"LateFusionModel class defined with DYNAMIC configuration\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =============================================================================\n# Cell 7: Late Fusion Trainer with Scalar Loss Fix\n# =============================================================================\n\nclass LateFusionTrainer:\n    \"\"\"\n    Proper trainer for Late Fusion model using Ultralytics YOLO loss.\n    Includes fix for scalar loss reduction.\n    \"\"\"\n    \n    def __init__(\n        self,\n        model: nn.Module,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        device: str = 'cuda',\n        epochs: int = 100,\n        patience: int = 30,\n        lr: float = 0.01,\n        momentum: float = 0.937,\n        weight_decay: float = 0.0005,\n        save_dir: Path = None,\n        num_classes: int = 1,\n    ):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.epochs = epochs\n        self.patience = patience\n        self.save_dir = Path(save_dir) if save_dir else Path('runs/fusion')\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.num_classes = num_classes\n        \n        # Only optimize trainable parameters\n        trainable_params = [p for p in model.parameters() if p.requires_grad]\n        self.optimizer = torch.optim.SGD(\n            trainable_params,\n            lr=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            nesterov=True,\n        )\n        \n        # Learning rate scheduler\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=epochs,\n            eta_min=lr * 0.01,\n        )\n        \n        # Mixed precision training\n        self.scaler = torch.amp.GradScaler('cuda')\n        \n        # Initialize YOLO loss function\n        self.criterion = self._create_loss_function()\n        \n        # Training state\n        self.best_fitness = 0.0\n        self.epochs_no_improve = 0\n        self.history = []\n        \n    def _create_loss_function(self):\n        \"\"\"\n        Create YOLO v8DetectionLoss function.\n        \"\"\"\n        from ultralytics.utils.loss import v8DetectionLoss\n        from types import SimpleNamespace\n        \n        # Wrapper class to make our model compatible with v8DetectionLoss\n        class DummyDetectionModel(nn.Module):\n            def __init__(self, actual_model, device):\n                super().__init__()\n                self.actual_model = actual_model\n                self.nc = actual_model.num_classes\n                \n                # Point to the detection head\n                # The Dynamic/Fixed LateFusionModel stores the head in self.detect\n                if hasattr(actual_model, 'detect'):\n                    self.model = [actual_model.detect]\n                else:\n                    raise AttributeError(\"Model must have a 'detect' attribute (Detection Head)\")\n\n                # Get stride\n                if hasattr(actual_model.detect, 'stride'):\n                    self.stride = actual_model.detect.stride\n                else:\n                    self.stride = torch.tensor([8., 16., 32.])\n            \n                # Hyperparameters for loss\n                self.args = SimpleNamespace(\n                    box=7.5,      # box loss gain\n                    cls=0.5,      # cls loss gain\n                    dfl=1.5,      # dfl loss gain\n                )\n            \n                self._dummy_param = nn.Parameter(torch.zeros(1, device=device))\n        \n            def parameters(self):\n                return iter([self._dummy_param] + list(self.actual_model.parameters()))\n    \n        dummy_model = DummyDetectionModel(self.model, self.device)\n        return v8DetectionLoss(dummy_model)\n    \n    def _prepare_batch(self, batch: Dict) -> Dict:\n        \"\"\"Prepare batch for YOLO loss.\"\"\"\n        rgb = batch['rgb'].to(self.device)\n        depth = batch['depth'].to(self.device)\n        labels = batch['labels'].to(self.device)\n        \n        if len(labels) > 0:\n            batch_idx = labels[:, 0].long()\n            cls = labels[:, 1].long()\n            bboxes = labels[:, 2:6]\n        else:\n            batch_idx = torch.zeros(0, dtype=torch.long, device=self.device)\n            cls = torch.zeros(0, dtype=torch.long, device=self.device)\n            bboxes = torch.zeros(0, 4, device=self.device)\n        \n        return {\n            'img': (rgb, depth),\n            'batch_idx': batch_idx,\n            'cls': cls,\n            'bboxes': bboxes,\n        }\n    \n    def train_epoch(self, epoch: int):\n        \"\"\"Train for one epoch.\"\"\"\n        self.model.train()\n        \n        # Keep backbones frozen\n        self.model.rgb_backbone.eval()\n        self.model.depth_backbone.eval()\n        \n        epoch_loss = 0.0\n        epoch_box_loss = 0.0\n        epoch_cls_loss = 0.0\n        epoch_dfl_loss = 0.0\n        \n        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n        \n        for batch_idx, batch in enumerate(pbar):\n            prepared_batch = self._prepare_batch(batch)\n            \n            self.optimizer.zero_grad()\n            \n            with torch.amp.autocast('cuda'):\n                rgb = prepared_batch['img'][0]\n                depth = prepared_batch['img'][1]\n                \n                preds = self.model(rgb, depth)\n                \n                # Compute YOLO loss\n                loss, loss_items = self.criterion(preds, prepared_batch)\n                \n                # --- FIX: Ensure loss is scalar for backward() ---\n                if loss.numel() > 1:\n                    loss = loss.sum()\n                # -------------------------------------------------\n                \n                box_loss = loss_items[0] if len(loss_items) > 0 else 0\n                cls_loss = loss_items[1] if len(loss_items) > 1 else 0\n                dfl_loss = loss_items[2] if len(loss_items) > 2 else 0\n            \n            # Backward pass\n            self.scaler.scale(loss).backward()\n            \n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)\n            \n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            # Track losses\n            epoch_loss += loss.item()\n            epoch_box_loss += box_loss.item() if torch.is_tensor(box_loss) else box_loss\n            epoch_cls_loss += cls_loss.item() if torch.is_tensor(cls_loss) else cls_loss\n            epoch_dfl_loss += dfl_loss.item() if torch.is_tensor(dfl_loss) else dfl_loss\n            \n            pbar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'box': f\"{box_loss.item() if torch.is_tensor(box_loss) else box_loss:.4f}\",\n                'cls': f\"{cls_loss.item() if torch.is_tensor(cls_loss) else cls_loss:.4f}\",\n            })\n        \n        num_batches = len(self.train_loader)\n        return {\n            'loss': epoch_loss / num_batches,\n            'box_loss': epoch_box_loss / num_batches,\n            'cls_loss': epoch_cls_loss / num_batches,\n            'dfl_loss': epoch_dfl_loss / num_batches,\n        }\n    \n    @torch.no_grad()\n    def validate(self):\n        \"\"\"Validate the model.\"\"\"\n        self.model.eval()\n        \n        val_loss = 0.0\n        val_box_loss = 0.0\n        val_cls_loss = 0.0\n        val_dfl_loss = 0.0\n        \n        for batch in self.val_loader:\n            prepared_batch = self._prepare_batch(batch)\n            rgb = prepared_batch['img'][0]\n            depth = prepared_batch['img'][1]\n            \n            preds = self.model(rgb, depth)\n            loss, loss_items = self.criterion(preds, prepared_batch)\n            \n            # Ensure scalar for logging\n            if loss.numel() > 1:\n                loss = loss.sum()\n            \n            val_loss += loss.item()\n            val_box_loss += loss_items[0].item() if len(loss_items) > 0 and torch.is_tensor(loss_items[0]) else 0\n            val_cls_loss += loss_items[1].item() if len(loss_items) > 1 and torch.is_tensor(loss_items[1]) else 0\n            val_dfl_loss += loss_items[2].item() if len(loss_items) > 2 and torch.is_tensor(loss_items[2]) else 0\n        \n        num_batches = len(self.val_loader)\n        return {\n            'loss': val_loss / num_batches,\n            'box_loss': val_box_loss / num_batches,\n            'cls_loss': val_cls_loss / num_batches,\n            'dfl_loss': val_dfl_loss / num_batches,\n        }\n    \n    def save_checkpoint(self, epoch: int, is_best: bool = False):\n        \"\"\"Save model checkpoint.\"\"\"\n        weights_dir = self.save_dir / 'weights'\n        weights_dir.mkdir(exist_ok=True)\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'best_fitness': self.best_fitness,\n        }\n        torch.save(checkpoint, weights_dir / 'last.pt')\n        if is_best:\n            torch.save(checkpoint, weights_dir / 'best.pt')\n            print(f\"  Saved best model (fitness: {self.best_fitness:.4f})\")\n    \n    def train(self):\n        \"\"\"Full training loop.\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"Starting Late Fusion Training\")\n        print(f\"Epochs: {self.epochs}\")\n        print(f\"Patience: {self.patience}\")\n        print(f\"Save directory: {self.save_dir}\")\n        print(f\"Device: {self.device}\")\n        print(f\"{'='*60}\\n\")\n        \n        for epoch in range(self.epochs):\n            train_metrics = self.train_epoch(epoch)\n            val_metrics = self.validate()\n            self.scheduler.step()\n            \n            fitness = 1.0 / (val_metrics['loss'] + 1e-6)\n            \n            is_best = fitness > self.best_fitness\n            if is_best:\n                self.best_fitness = fitness\n                self.epochs_no_improve = 0\n            else:\n                self.epochs_no_improve += 1\n            \n            self.save_checkpoint(epoch, is_best)\n            \n            self.history.append({\n                'epoch': epoch + 1,\n                'train_loss': train_metrics['loss'],\n                'val_loss': val_metrics['loss'],\n                'fitness': fitness,\n            })\n            \n            print(f\"\\nEpoch {epoch+1}/{self.epochs}:\")\n            print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n            print(f\"  Val Loss:   {val_metrics['loss']:.4f}\")\n            print(f\"  Fitness:    {fitness:.4f} {'*' if is_best else ''}\")\n            \n            if self.epochs_no_improve >= self.patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n        \n        print(f\"Training complete!\")\n        return self.history\n\nprint(\"LateFusionTrainer class defined with SCALAR LOSS fix\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.804253Z","iopub.execute_input":"2026-01-29T02:05:07.804505Z","iopub.status.idle":"2026-01-29T02:05:07.831520Z","shell.execute_reply.started":"2026-01-29T02:05:07.804480Z","shell.execute_reply":"2026-01-29T02:05:07.830883Z"}},"outputs":[{"name":"stdout","text":"LateFusionTrainer class defined with SCALAR LOSS fix\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =============================================================================\n# Cell 8: Find Best Weights from A.1 and A.2\n# =============================================================================\n\n# Direct paths to best.pt files\nif IS_KAGGLE:\n    rgb_weights = '/kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt'\n    depth_weights = '/kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt'\nelse:\n    rgb_weights = str(RGB_WEIGHTS_DIR / 'best.pt')\n    depth_weights = str(DEPTH_WEIGHTS_DIR / 'best.pt')\n\nprint(\"Pre-trained Weights:\")\nprint(f\"  RGB (A.1):   {rgb_weights}\")\nprint(f\"  Depth (A.2): {depth_weights}\")\n\n# Check if weights exist\nrgb_exists = Path(rgb_weights).exists()\ndepth_exists = Path(depth_weights).exists()\n\nprint(f\"\\nWeight File Verification:\")\nprint(f\"  RGB weights:   {'OK' if rgb_exists else 'NOT FOUND'}\")\nprint(f\"  Depth weights: {'OK' if depth_exists else 'NOT FOUND'}\")\n\nif not rgb_exists or not depth_exists:\n    print(\"\\nWARNING: Pre-trained weights not found!\")\n    print(\"Please ensure A.1 and A.2 experiments have been uploaded as Kaggle Models.\")\n    \n    # Fallback: use pretrained yolo11n.pt\n    print(\"\\nFallback: Using yolo11n.pt for both backbones\")\n    rgb_weights = 'yolo11n.pt'\n    depth_weights = 'yolo11n.pt'\nelse:\n    print(\"\\nWeights found successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.832495Z","iopub.execute_input":"2026-01-29T02:05:07.832761Z","iopub.status.idle":"2026-01-29T02:05:07.847445Z","shell.execute_reply.started":"2026-01-29T02:05:07.832704Z","shell.execute_reply":"2026-01-29T02:05:07.846779Z"}},"outputs":[{"name":"stdout","text":"Pre-trained Weights:\n  RGB (A.1):   /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\n  Depth (A.2): /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nWeight File Verification:\n  RGB weights:   OK\n  Depth weights: OK\n\nWeights found successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# =============================================================================\n# Cell 9: Training Loop (5 Seeds) with Proper YOLO Loss\n# =============================================================================\n\nresults_all = {}\ntraining_times = {}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING LOOP - A.5 LATE FUSION\")\nprint(\"=\"*60)\nprint(\"\\nKey Features:\")\nprint(\"  - Multi-scale feature fusion (P3, P4, P5)\")\nprint(\"  - Proper YOLO v8DetectionLoss (box + cls + dfl)\")\nprint(\"  - Frozen RGB and Depth backbones\")\nprint(\"  - Trainable fusion layers and detection head\")\nprint(\"=\"*60)\n\nfor idx, seed in enumerate(SEEDS, 1):\n    start_time = time.time()\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"TRAINING A.5 LATE FUSION - Seed {seed} ({idx}/{len(SEEDS)})\")\n    print(f\"{'='*60}\")\n    \n    # Set random seeds\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    \n    try:\n        # Create datasets\n        train_dataset = LateFusionDataset(\n            rgb_img_dir=RGB_DATASET / 'images' / 'train',\n            depth_img_dir=DEPTH_DATASET / 'images' / 'train',\n            label_dir=RGB_DATASET / 'labels' / 'train',\n            img_size=IMGSZ,\n            augment=True,\n            augment_params=AUGMENT_PARAMS\n        )\n        \n        val_dataset = LateFusionDataset(\n            rgb_img_dir=RGB_DATASET / 'images' / 'val',\n            depth_img_dir=DEPTH_DATASET / 'images' / 'val',\n            label_dir=RGB_DATASET / 'labels' / 'val',\n            img_size=IMGSZ,\n            augment=False\n        )\n        \n        # Create dataloaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=True,\n            num_workers=NUM_WORKERS,\n            collate_fn=collate_fn,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False,\n            num_workers=NUM_WORKERS,\n            collate_fn=collate_fn,\n            pin_memory=True\n        )\n        \n        print(f\"\\nDataset loaded:\")\n        print(f\"  Train samples: {len(train_dataset)}\")\n        print(f\"  Val samples: {len(val_dataset)}\")\n        \n        # Create model\n        model = LateFusionModel(\n            rgb_model_path=rgb_weights,\n            depth_model_path=depth_weights,\n            num_classes=1,\n            device=DEVICE\n        )\n        \n        # Create save directory\n        save_dir = RUNS_PATH / f\"{EXP_PREFIX}_seed{seed}\"\n        \n        # Create trainer with proper YOLO loss\n        trainer = LateFusionTrainer(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            device=DEVICE,\n            epochs=EPOCHS,\n            patience=PATIENCE,\n            lr=0.01,\n            save_dir=save_dir,\n            num_classes=1,\n        )\n        \n        # Train\n        history = trainer.train()\n        \n        elapsed = time.time() - start_time\n        training_times[seed] = elapsed\n        \n        results_all[seed] = {\n            'model_path': str(save_dir / 'weights' / 'best.pt'),\n            'history': history,\n            'completed': True,\n            'best_fitness': trainer.best_fitness,\n        }\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Seed {seed} completed successfully!\")\n        print(f\"Best fitness: {trainer.best_fitness:.4f}\")\n        print(f\"Training time: {elapsed/60:.1f} minutes\")\n        print(f\"{'='*60}\")\n        \n    except Exception as e:\n        print(f\"\\n{'='*60}\")\n        print(f\"Seed {seed} failed: {e}\")\n        print(f\"{'='*60}\")\n        import traceback\n        traceback.print_exc()\n        results_all[seed] = {'error': str(e), 'completed': False}\n    \n    finally:\n        # Cleanup\n        if 'model' in locals():\n            del model\n        if 'trainer' in locals():\n            del trainer\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING LOOP COMPLETED\")\nprint(f\"Successful: {sum(1 for r in results_all.values() if r.get('completed', False))}/{len(SEEDS)}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:05:07.848342Z","iopub.execute_input":"2026-01-29T02:05:07.848607Z","iopub.status.idle":"2026-01-29T02:16:06.236996Z","shell.execute_reply.started":"2026-01-29T02:05:07.848575Z","shell.execute_reply":"2026-01-29T02:16:06.236275Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSTARTING TRAINING LOOP - A.5 LATE FUSION\n============================================================\n\nKey Features:\n  - Multi-scale feature fusion (P3, P4, P5)\n  - Proper YOLO v8DetectionLoss (box + cls + dfl)\n  - Frozen RGB and Depth backbones\n  - Trainable fusion layers and detection head\n============================================================\n\n============================================================\nTRAINING A.5 LATE FUSION - Seed 42 (1/5)\n============================================================\n[LateFusionDataset] Loaded 280 valid samples\n[LateFusionDataset] Loaded 80 valid samples\n\nDataset loaded:\n  Train samples: 280\n  Val samples: 80\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n\n============================================================\nStarting Late Fusion Training\nEpochs: 5\nPatience: 30\nSave directory: /kaggle/working/runs/detect/exp_a5_fusion_seed42\nDevice: 0\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f89fb52b7dd4f2e98e55b6ae24339ec"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0038)\n\nEpoch 1/5:\n  Train Loss: 142.0829\n  Val Loss:   266.5819\n  Fitness:    0.0038 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da6368f3f0524b8b86536f2078ae6fbd"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/5:\n  Train Loss: 102.2985\n  Val Loss:   377.0908\n  Fitness:    0.0027 \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb7c684e3ca4d39ab2f676e0644063d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0070)\n\nEpoch 3/5:\n  Train Loss: 94.7016\n  Val Loss:   143.0474\n  Fitness:    0.0070 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1977923252db4c76ac275e61dcadee2d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0073)\n\nEpoch 4/5:\n  Train Loss: 88.6269\n  Val Loss:   137.8427\n  Fitness:    0.0073 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120d60056efa447f99ac8a817a4a83b4"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0095)\n\nEpoch 5/5:\n  Train Loss: 87.1874\n  Val Loss:   105.0882\n  Fitness:    0.0095 *\nTraining complete!\n\n============================================================\nSeed 42 completed successfully!\nBest fitness: 0.0095\nTraining time: 2.5 minutes\n============================================================\n\n============================================================\nTRAINING A.5 LATE FUSION - Seed 123 (2/5)\n============================================================\n[LateFusionDataset] Loaded 280 valid samples\n[LateFusionDataset] Loaded 80 valid samples\n\nDataset loaded:\n  Train samples: 280\n  Val samples: 80\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n\n============================================================\nStarting Late Fusion Training\nEpochs: 5\nPatience: 30\nSave directory: /kaggle/working/runs/detect/exp_a5_fusion_seed123\nDevice: 0\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5ded453b32c48d38151e6de75f029da"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0037)\n\nEpoch 1/5:\n  Train Loss: 131.9920\n  Val Loss:   272.7723\n  Fitness:    0.0037 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dad5a2a8879047a683b486218c6e9d4e"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0052)\n\nEpoch 2/5:\n  Train Loss: 87.9219\n  Val Loss:   192.3286\n  Fitness:    0.0052 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f5bbbba0d247c096b11d59b97b5054"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0084)\n\nEpoch 3/5:\n  Train Loss: 82.8933\n  Val Loss:   119.6129\n  Fitness:    0.0084 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c720ca33f6415c969db446f51390e5"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0103)\n\nEpoch 4/5:\n  Train Loss: 77.9450\n  Val Loss:   96.9769\n  Fitness:    0.0103 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f138b369c680443eae3000de3b07f6c3"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0120)\n\nEpoch 5/5:\n  Train Loss: 75.6404\n  Val Loss:   83.3389\n  Fitness:    0.0120 *\nTraining complete!\n\n============================================================\nSeed 123 completed successfully!\nBest fitness: 0.0120\nTraining time: 1.9 minutes\n============================================================\n\n============================================================\nTRAINING A.5 LATE FUSION - Seed 456 (3/5)\n============================================================\n[LateFusionDataset] Loaded 280 valid samples\n[LateFusionDataset] Loaded 80 valid samples\n\nDataset loaded:\n  Train samples: 280\n  Val samples: 80\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n\n============================================================\nStarting Late Fusion Training\nEpochs: 5\nPatience: 30\nSave directory: /kaggle/working/runs/detect/exp_a5_fusion_seed456\nDevice: 0\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e91bb0523f664e4fa23fa0e0b747a394"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0042)\n\nEpoch 1/5:\n  Train Loss: 154.2834\n  Val Loss:   238.1534\n  Fitness:    0.0042 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8260bd3d976d45fdb1e74f8736f5b4c9"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0074)\n\nEpoch 2/5:\n  Train Loss: 104.7364\n  Val Loss:   135.9540\n  Fitness:    0.0074 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f259cc15b2324048add8576b8698dbf5"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0096)\n\nEpoch 3/5:\n  Train Loss: 94.7262\n  Val Loss:   104.2322\n  Fitness:    0.0096 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2cf3ae55004e0e8943c91f201bc17c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0100)\n\nEpoch 4/5:\n  Train Loss: 88.5682\n  Val Loss:   100.3098\n  Fitness:    0.0100 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"325ca629546043279dbbf99f7a7fe2c4"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0109)\n\nEpoch 5/5:\n  Train Loss: 85.9340\n  Val Loss:   91.7427\n  Fitness:    0.0109 *\nTraining complete!\n\n============================================================\nSeed 456 completed successfully!\nBest fitness: 0.0109\nTraining time: 1.9 minutes\n============================================================\n\n============================================================\nTRAINING A.5 LATE FUSION - Seed 789 (4/5)\n============================================================\n[LateFusionDataset] Loaded 280 valid samples\n[LateFusionDataset] Loaded 80 valid samples\n\nDataset loaded:\n  Train samples: 280\n  Val samples: 80\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n\n============================================================\nStarting Late Fusion Training\nEpochs: 5\nPatience: 30\nSave directory: /kaggle/working/runs/detect/exp_a5_fusion_seed789\nDevice: 0\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa7763ce7f9459692065839a1f40981"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0067)\n\nEpoch 1/5:\n  Train Loss: 130.9025\n  Val Loss:   150.2955\n  Fitness:    0.0067 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53aeb3903ffb4fb29e98f66fe7e94370"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0092)\n\nEpoch 2/5:\n  Train Loss: 90.4130\n  Val Loss:   108.9332\n  Fitness:    0.0092 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f08266c6c264fb3b4a886a8adb2d659"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0098)\n\nEpoch 3/5:\n  Train Loss: 82.9109\n  Val Loss:   101.5857\n  Fitness:    0.0098 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2af5578171242ff9b3fc96c5976deec"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0112)\n\nEpoch 4/5:\n  Train Loss: 79.9094\n  Val Loss:   89.5395\n  Fitness:    0.0112 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca32aeaab41c4adfa0470d1942c585ac"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0118)\n\nEpoch 5/5:\n  Train Loss: 76.8228\n  Val Loss:   84.5092\n  Fitness:    0.0118 *\nTraining complete!\n\n============================================================\nSeed 789 completed successfully!\nBest fitness: 0.0118\nTraining time: 1.9 minutes\n============================================================\n\n============================================================\nTRAINING A.5 LATE FUSION - Seed 101 (5/5)\n============================================================\n[LateFusionDataset] Loaded 280 valid samples\n[LateFusionDataset] Loaded 80 valid samples\n\nDataset loaded:\n  Train samples: 280\n  Val samples: 80\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n\n============================================================\nStarting Late Fusion Training\nEpochs: 5\nPatience: 30\nSave directory: /kaggle/working/runs/detect/exp_a5_fusion_seed101\nDevice: 0\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93157644cd974b3ca0bdf5d7ee3956be"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0055)\n\nEpoch 1/5:\n  Train Loss: 125.7577\n  Val Loss:   181.6531\n  Fitness:    0.0055 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e04a24c9458a40f2919f008544c10686"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0075)\n\nEpoch 2/5:\n  Train Loss: 88.3202\n  Val Loss:   132.6949\n  Fitness:    0.0075 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb36a0707bb4c4baf906469c08d27df"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0098)\n\nEpoch 3/5:\n  Train Loss: 81.7482\n  Val Loss:   102.2691\n  Fitness:    0.0098 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efabacdb713e48718bf4ef9ce78e4625"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0113)\n\nEpoch 4/5:\n  Train Loss: 76.7035\n  Val Loss:   88.3794\n  Fitness:    0.0113 *\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa74aabdf8c94f83ba9dbf999d1b4f7d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/train/rgb_0173.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n/tmp/ipykernel_55/1689061521.py:136: UserWarning: loadtxt: input contained no data: \"/kaggle/input/ffb-localization-dataset/ffb_localization/labels/val/rgb_0063.txt\"\n  labels = np.loadtxt(str(label_path), ndmin=2).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"  Saved best model (fitness: 0.0122)\n\nEpoch 5/5:\n  Train Loss: 74.4893\n  Val Loss:   82.0628\n  Fitness:    0.0122 *\nTraining complete!\n\n============================================================\nSeed 101 completed successfully!\nBest fitness: 0.0122\nTraining time: 2.1 minutes\n============================================================\n\n============================================================\nTRAINING LOOP COMPLETED\nSuccessful: 5/5\n============================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# =============================================================================\n# Cell 10: mAP Evaluation Functions (Proper Implementation)\n# =============================================================================\n\ndef box_iou(box1, box2):\n    \"\"\"\n    Compute IoU between two sets of boxes.\n    box1: [N, 4] in xyxy format\n    box2: [M, 4] in xyxy format\n    Returns: [N, M] IoU matrix\n    \"\"\"\n    def box_area(box):\n        return (box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1])\n\n    area1 = box_area(box1)\n    area2 = box_area(box2)\n\n    # Intersection\n    lt = torch.max(box1[:, None, :2], box2[None, :, :2])\n    rb = torch.min(box1[:, None, 2:], box2[None, :, 2:])\n    wh = (rb - lt).clamp(min=0)\n    inter = wh[:, :, 0] * wh[:, :, 1]\n\n    # Union\n    union = area1[:, None] + area2[None, :] - inter\n\n    return inter / (union + 1e-6)\n\n\ndef xywh_to_xyxy(boxes, img_size=640):\n    \"\"\"Convert boxes from xywh normalized to xyxy pixel format.\"\"\"\n    if len(boxes) == 0:\n        return torch.zeros((0, 4))\n\n    boxes = torch.tensor(boxes) if not isinstance(boxes, torch.Tensor) else boxes\n    x_center = boxes[:, 0] * img_size\n    y_center = boxes[:, 1] * img_size\n    w = boxes[:, 2] * img_size\n    h = boxes[:, 3] * img_size\n\n    x1 = x_center - w / 2\n    y1 = y_center - h / 2\n    x2 = x_center + w / 2\n    y2 = y_center + h / 2\n\n    return torch.stack([x1, y1, x2, y2], dim=1)\n\n\ndef compute_ap(recalls, precisions):\n    \"\"\"Compute Average Precision using 101-point interpolation (COCO style).\"\"\"\n    recalls = np.concatenate([[0.0], recalls, [1.0]])\n    precisions = np.concatenate([[1.0], precisions, [0.0]])\n\n    # Ensure precision is monotonically decreasing\n    for i in range(len(precisions) - 2, -1, -1):\n        precisions[i] = max(precisions[i], precisions[i + 1])\n\n    # 101-point interpolation\n    recall_levels = np.linspace(0, 1, 101)\n    ap = 0.0\n    for r in recall_levels:\n        prec_at_r = precisions[recalls >= r]\n        if len(prec_at_r) > 0:\n            ap += prec_at_r.max()\n\n    return ap / 101\n\n\ndef evaluate_detections(all_predictions, all_targets, iou_threshold=0.5, img_size=640):\n    \"\"\"\n    Evaluate detections and compute Precision, Recall, and AP.\n\n    Args:\n        all_predictions: List of (boxes, scores) per image, boxes in xywh normalized\n        all_targets: List of target boxes per image, in xywh normalized\n        iou_threshold: IoU threshold for matching\n\n    Returns:\n        dict with Precision, Recall, AP\n    \"\"\"\n    all_scores = []\n    all_matches = []  # 1 if TP, 0 if FP\n    total_gt = 0\n\n    for preds, targets in zip(all_predictions, all_targets):\n        pred_boxes, pred_scores = preds\n\n        if len(pred_boxes) == 0:\n            total_gt += len(targets)\n            continue\n\n        # Convert to xyxy\n        pred_xyxy = xywh_to_xyxy(pred_boxes, img_size)\n\n        if len(targets) == 0:\n            # All predictions are FP\n            for score in pred_scores:\n                all_scores.append(score)\n                all_matches.append(0)\n            continue\n\n        target_xyxy = xywh_to_xyxy(targets, img_size)\n        total_gt += len(targets)\n\n        # Compute IoU matrix\n        ious = box_iou(pred_xyxy, target_xyxy)\n\n        # Match predictions to targets (greedy matching)\n        matched_gt = set()\n\n        # Sort predictions by score (descending)\n        sorted_indices = np.argsort(pred_scores)[::-1]\n\n        for idx in sorted_indices:\n            score = pred_scores[idx]\n            all_scores.append(score)\n\n            if len(matched_gt) == len(targets):\n                all_matches.append(0)\n                continue\n\n            # Find best matching GT\n            iou_row = ious[idx].cpu().numpy()\n            best_gt_idx = -1\n            best_iou = iou_threshold\n\n            for gt_idx in range(len(targets)):\n                if gt_idx in matched_gt:\n                    continue\n                if iou_row[gt_idx] > best_iou:\n                    best_iou = iou_row[gt_idx]\n                    best_gt_idx = gt_idx\n\n            if best_gt_idx >= 0:\n                all_matches.append(1)  # TP\n                matched_gt.add(best_gt_idx)\n            else:\n                all_matches.append(0)  # FP\n\n    if len(all_scores) == 0 or total_gt == 0:\n        return {'Precision': 0.0, 'Recall': 0.0, 'AP': 0.0}\n\n    # Sort by score\n    sorted_indices = np.argsort(all_scores)[::-1]\n    all_matches = np.array(all_matches)[sorted_indices]\n\n    # Compute cumulative TP and FP\n    tp_cumsum = np.cumsum(all_matches)\n    fp_cumsum = np.cumsum(1 - all_matches)\n\n    # Precision and Recall at each threshold\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n    recalls = tp_cumsum / (total_gt + 1e-6)\n\n    # Final precision and recall\n    final_precision = float(precisions[-1]) if len(precisions) > 0 else 0.0\n    final_recall = float(recalls[-1]) if len(recalls) > 0 else 0.0\n\n    # Compute AP\n    ap = compute_ap(recalls, precisions)\n\n    return {\n        'Precision': final_precision,\n        'Recall': final_recall,\n        'AP': float(ap)\n    }\n\n\ndef nms_numpy(boxes, scores, iou_threshold=0.45):\n    \"\"\"\n    Simple NMS implementation.\n    boxes: [N, 4] in xyxy format (numpy)\n    scores: [N] (numpy)\n    Returns: indices to keep\n    \"\"\"\n    if len(boxes) == 0:\n        return np.array([], dtype=int)\n\n    boxes = np.array(boxes)\n    scores = np.array(scores)\n\n    # Sort by score\n    order = scores.argsort()[::-1]\n    keep = []\n\n    while len(order) > 0:\n        i = order[0]\n        keep.append(i)\n\n        if len(order) == 1:\n            break\n\n        # Compute IoU with rest\n        xx1 = np.maximum(boxes[i, 0], boxes[order[1:], 0])\n        yy1 = np.maximum(boxes[i, 1], boxes[order[1:], 1])\n        xx2 = np.minimum(boxes[i, 2], boxes[order[1:], 2])\n        yy2 = np.minimum(boxes[i, 3], boxes[order[1:], 3])\n\n        w = np.maximum(0, xx2 - xx1)\n        h = np.maximum(0, yy2 - yy1)\n        inter = w * h\n\n        area_i = (boxes[i, 2] - boxes[i, 0]) * (boxes[i, 3] - boxes[i, 1])\n        area_rest = (boxes[order[1:], 2] - boxes[order[1:], 0]) * (boxes[order[1:], 3] - boxes[order[1:], 1])\n\n        iou = inter / (area_i + area_rest - inter + 1e-6)\n\n        # Keep boxes with IoU < threshold\n        inds = np.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n\n    return np.array(keep)\n\n\ndef decode_yolo_output(output, conf_threshold=0.25, iou_threshold=0.45, img_size=640):\n    \"\"\"\n    Decode YOLO output to boxes and scores.\n    \n    Args:\n        output: Raw model output tensor\n        conf_threshold: Confidence threshold\n        iou_threshold: NMS IoU threshold\n        img_size: Image size\n        \n    Returns:\n        boxes: [N, 4] in xywh normalized format\n        scores: [N] confidence scores\n    \"\"\"\n    # Handle different output formats\n    if isinstance(output, (list, tuple)):\n        output = output[0]\n    \n    # Expected shape: [batch, num_classes + 4, num_anchors] or [batch, num_anchors, num_classes + 4]\n    if output.dim() == 3:\n        if output.shape[1] == 5:  # [batch, 5, anchors] -> 1 class + 4 coords\n            output = output.permute(0, 2, 1)  # [batch, anchors, 5]\n        elif output.shape[2] == 5:  # Already [batch, anchors, 5]\n            pass\n        else:\n            # Try to reshape\n            output = output.view(output.shape[0], -1, 5)\n    \n    all_boxes = []\n    all_scores = []\n    \n    for batch_idx in range(output.shape[0]):\n        pred = output[batch_idx]  # [anchors, 5] or similar\n        \n        if pred.dim() == 1:\n            pred = pred.unsqueeze(0)\n        \n        # Assume format: x, y, w, h, conf (or conf first)\n        if pred.shape[-1] >= 5:\n            # Try conf at last position\n            conf = pred[:, 4] if pred.shape[-1] >= 5 else pred[:, 0]\n            boxes = pred[:, :4]\n        else:\n            continue\n        \n        # Filter by confidence\n        mask = conf > conf_threshold\n        conf = conf[mask]\n        boxes = boxes[mask]\n        \n        if len(boxes) == 0:\n            all_boxes.append(np.zeros((0, 4)))\n            all_scores.append(np.array([]))\n            continue\n        \n        # Convert to numpy\n        boxes_np = boxes.cpu().numpy()\n        scores_np = conf.cpu().numpy()\n        \n        # Ensure boxes are in valid range [0, 1]\n        boxes_np = np.clip(boxes_np, 0, 1)\n        \n        # Convert to xyxy for NMS\n        x_center = boxes_np[:, 0] * img_size\n        y_center = boxes_np[:, 1] * img_size\n        w = boxes_np[:, 2] * img_size\n        h = boxes_np[:, 3] * img_size\n        \n        x1 = x_center - w / 2\n        y1 = y_center - h / 2\n        x2 = x_center + w / 2\n        y2 = y_center + h / 2\n        \n        boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)\n        \n        # Apply NMS\n        keep = nms_numpy(boxes_xyxy, scores_np, iou_threshold)\n        \n        if len(keep) > 0:\n            all_boxes.append(boxes_np[keep])\n            all_scores.append(scores_np[keep])\n        else:\n            all_boxes.append(np.zeros((0, 4)))\n            all_scores.append(np.array([]))\n    \n    return all_boxes, all_scores\n\nprint(\"mAP evaluation functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:17:00.677785Z","iopub.execute_input":"2026-01-29T02:17:00.678125Z","iopub.status.idle":"2026-01-29T02:17:00.707509Z","shell.execute_reply.started":"2026-01-29T02:17:00.678093Z","shell.execute_reply":"2026-01-29T02:17:00.706656Z"}},"outputs":[{"name":"stdout","text":"mAP evaluation functions defined\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# =============================================================================\n# Cell 11: Evaluation on Test Set (FINAL FIX: SCALING CORRECTED)\n# =============================================================================\n\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom ultralytics.utils.ops import xywh2xyxy\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndef evaluate_on_test(test_loader, seeds=[42, 123, 456, 789, 101]):\n    print(\"\\n\" + \"=\"*60)\n    print(\"EVALUATION ON TEST SET (mAP)\")\n    print(\"=\"*60)\n    print(f\"Test samples: {len(test_loader.dataset)}\")\n    \n    results = {}\n    \n    for seed in seeds:\n        print(f\"\\nEvaluating Seed {seed}...\")\n        \n        try:\n            # 1. SETUP MAP LOCATION\n            if isinstance(DEVICE, int):\n                map_loc = f'cuda:{DEVICE}'\n            elif str(DEVICE) == '0':\n                map_loc = 'cuda:0'\n            else:\n                map_loc = DEVICE\n            \n            # 2. SETUP PATHS\n            if IS_KAGGLE:\n                rgb_path = f'/kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt'\n                depth_path = f'/kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt'\n            else:\n                rgb_path = str(RGB_WEIGHTS_DIR / 'best.pt')\n                depth_path = str(DEPTH_WEIGHTS_DIR / 'best.pt')\n                \n            weights_path = RUNS_PATH / f\"{EXP_PREFIX}_seed{seed}\" / 'weights' / 'best.pt'\n            \n            if not weights_path.exists():\n                print(f\"  > Weights not found: {weights_path}\")\n                continue\n\n            # 3. INITIALIZE MODEL\n            model = LateFusionModel(\n                rgb_model_path=rgb_path,\n                depth_model_path=depth_path,\n                num_classes=1,\n                device=DEVICE\n            )\n            \n            # 4. LOAD WEIGHTS\n            checkpoint = torch.load(weights_path, map_location=map_loc)\n            if 'model_state_dict' in checkpoint:\n                state_dict = checkpoint['model_state_dict']\n            else:\n                state_dict = checkpoint\n            model.load_state_dict(state_dict)\n            model.to(DEVICE)\n            model.eval()\n            \n            # 5. METRIC SETUP\n            metric = MeanAveragePrecision(iou_type=\"bbox\")\n            \n            all_preds = []\n            all_targets = []\n            \n            # 6. INFERENCE LOOP\n            for batch in tqdm(test_loader, desc=f\"Seed {seed}\"):\n                rgb = batch['rgb'].to(DEVICE)\n                depth = batch['depth'].to(DEVICE)\n                labels = batch['labels'].to(DEVICE)\n                \n                curr_bs = rgb.shape[0]\n                h, w = rgb.shape[2], rgb.shape[3]\n                \n                # --- PROCESS TARGETS (TARGETS ARE NORMALIZED 0-1) ---\n                batch_targets = []\n                for i in range(curr_bs):\n                    mask = labels[:, 0] == i\n                    img_labels = labels[mask]\n                    \n                    if len(img_labels) > 0:\n                        cls = img_labels[:, 1].long()\n                        boxes_norm = img_labels[:, 2:]\n                        \n                        # Convert Target: Norm XYWH -> Absolute XYXY\n                        # TARGETS PERLU DIKALI W/H\n                        boxes_abs = xywh2xyxy(boxes_norm).clone()\n                        boxes_abs[:, 0] *= w\n                        boxes_abs[:, 1] *= h\n                        boxes_abs[:, 2] *= w\n                        boxes_abs[:, 3] *= h\n                        \n                        batch_targets.append(dict(boxes=boxes_abs, labels=cls))\n                    else:\n                        batch_targets.append(dict(\n                            boxes=torch.zeros((0, 4), device=DEVICE),\n                            labels=torch.zeros(0, dtype=torch.long, device=DEVICE)\n                        ))\n                all_targets.extend(batch_targets)\n                \n                # --- PROCESS PREDICTIONS (PREDS ARE ALREADY ABSOLUTE PIXELS) ---\n                with torch.no_grad():\n                    raw_preds = model(rgb, depth)\n                    \n                    if isinstance(raw_preds, tuple):\n                        preds_to_process = raw_preds[0] \n                    else:\n                        preds_to_process = raw_preds\n\n                    # [B, 5, 8400] -> [B, 8400, 5]\n                    if preds_to_process.shape[1] < preds_to_process.shape[2]:\n                         preds_to_process = preds_to_process.permute(0, 2, 1)\n                    \n                    batch_preds = []\n                    for i in range(curr_bs):\n                        p = preds_to_process[i] # [Anchors, 5]\n                        \n                        # Filter low confidence\n                        # Index 4 is confidence for single class\n                        conf_mask = p[:, 4] > 0.001 \n                        p = p[conf_mask]\n                        \n                        if len(p) == 0:\n                            batch_preds.append(dict(\n                                boxes=torch.zeros((0, 4), device=DEVICE),\n                                scores=torch.zeros(0, device=DEVICE),\n                                labels=torch.zeros(0, dtype=torch.long, device=DEVICE)\n                            ))\n                            continue\n                            \n                        boxes_xywh = p[:, :4]\n                        scores = p[:, 4]\n                        \n                        # Convert Preds: Absolute XYWH -> Absolute XYXY\n                        # PREDICTIONS TIDAK PERLU DIKALI W/H (Sudah Pixel)\n                        boxes_xyxy = xywh2xyxy(boxes_xywh)\n                        \n                        # Clip boxes to image boundaries (safety)\n                        boxes_xyxy[:, 0].clamp_(0, w)\n                        boxes_xyxy[:, 1].clamp_(0, h)\n                        boxes_xyxy[:, 2].clamp_(0, w)\n                        boxes_xyxy[:, 3].clamp_(0, h)\n                        \n                        # NMS\n                        from torchvision.ops import nms\n                        keep = nms(boxes_xyxy, scores, iou_threshold=0.6)\n                        \n                        batch_preds.append(dict(\n                            boxes=boxes_xyxy[keep],\n                            scores=scores[keep],\n                            labels=torch.zeros_like(scores[keep], dtype=torch.long)\n                        ))\n                    all_preds.extend(batch_preds)\n            \n            metric.update(all_preds, all_targets)\n            m = metric.compute()\n            \n            results[seed] = {\n                'mAP50': m['map_50'].item(),\n                'mAP50-95': m['map'].item()\n            }\n            print(f\"  > Result: mAP50={m['map_50'].item():.4f}\")\n\n        except Exception as e:\n            print(f\"  > Error evaluating seed {seed}: {e}\")\n            import traceback\n            traceback.print_exc()\n            \n    return results\n\n# --- MAIN EXECUTION ---\nif 'LateFusionDataset' not in locals():\n    print(\"Error: Dataset class not found. Run previous cells.\")\nelse:\n    # 1. CREATE TEST LOADER\n    print(\"Creating Test Loader...\")\n    test_dataset = LateFusionDataset(\n        rgb_img_dir=RGB_DATASET / 'images' / 'test',\n        depth_img_dir=DEPTH_DATASET / 'images' / 'test',\n        label_dir=RGB_DATASET / 'labels' / 'test',\n        img_size=IMGSZ,\n        augment=False\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        collate_fn=collate_fn\n    )\n\n    # 2. RUN EVALUATION\n    results_dict = evaluate_on_test(test_loader, SEEDS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:19:53.099342Z","iopub.execute_input":"2026-01-29T02:19:53.100158Z","iopub.status.idle":"2026-01-29T02:21:00.653048Z","shell.execute_reply.started":"2026-01-29T02:19:53.100115Z","shell.execute_reply":"2026-01-29T02:21:00.652153Z"}},"outputs":[{"name":"stdout","text":"Creating Test Loader...\n[LateFusionDataset] Loaded 40 valid samples\n\n============================================================\nEVALUATION ON TEST SET (mAP)\n============================================================\nTest samples: 40\n\nEvaluating Seed 42...\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed 42:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"decb710f2d824ddf98fc703d89640199"}},"metadata":{}},{"name":"stdout","text":"  > Result: mAP50=0.5974\n\nEvaluating Seed 123...\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed 123:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7497026c26c84ade9cb8a4b2d73fe8ac"}},"metadata":{}},{"name":"stdout","text":"  > Result: mAP50=0.7554\n\nEvaluating Seed 456...\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed 456:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aca2319e53874c988590e387ef45b62c"}},"metadata":{}},{"name":"stdout","text":"  > Result: mAP50=0.5618\n\nEvaluating Seed 789...\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed 789:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855a54532059402abdce223e6ac9477c"}},"metadata":{}},{"name":"stdout","text":"  > Result: mAP50=0.7441\n\nEvaluating Seed 101...\n\n============================================================\nInitializing Late Fusion Model (Dynamic Configuration)\n============================================================\n\nLoading RGB backbone from: /kaggle/input/exp-a1-rgb-v2-seed42/pytorch/default/1/best.pt\nLoading Depth backbone from: /kaggle/input/exp-a2-depth-v2-seed456/pytorch/default/1/best.pt\n\nAnalyzing backbone output shapes...\n  Backbone Output Channels: P3=128, P4=128, P5=256\n  WARNING: head.ch not found, inferring from cv2 layers...\n  Head Input Requirements:  P3=64, P4=128, P5=256\n\nConfiguring Fusion Layers:\n  Fusion P3: 256 -> 64\n  Fusion P4: 256 -> 128\n  Fusion P5: 512 -> 256\n\n[Parameter Count]\n  Total:      5,792,057\n  Trainable:  611,987 (10.6%)\n  Frozen:     5,180,070 (89.4%)\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Seed 101:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9818d9cdfe4cd1a190e772a4d3be15"}},"metadata":{}},{"name":"stdout","text":"  > Result: mAP50=0.7122\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# =============================================================================\n# Cell 12: Results Summary (Same Format as A.1-A.4b)\n# =============================================================================\n\nif results_dict:\n    df = pd.DataFrame(results_dict).T\n    df.index.name = 'Seed'\n    \n    # Calculate statistics\n    avg = df.mean()\n    std = df.std()\n    min_vals = df.min()\n    max_vals = df.max()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"A.5 LATE FUSION (V2) - FINAL RESULTS\")\n    print(\"=\"*60 + \"\\n\")\n    \n    print(\"Per-Seed Results:\")\n    print(df.to_string(float_format=lambda x: f\"{x:.4f}\"))\n    \n    print(\"\\n\" + \"-\"*60)\n    print(\"STATISTICAL SUMMARY\")\n    print(\"-\"*60)\n    print(f\"{'Metric':<15} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n    print(\"-\"*60)\n    for col in df.columns:\n        print(f\"{col:<15} {avg[col]:>10.4f} {std[col]:>10.4f} {min_vals[col]:>10.4f} {max_vals[col]:>10.4f}\")\n    \n    # Best seed\n    best_seed = df['mAP50'].idxmax()\n    print(f\"\\nBest Seed: {best_seed} (mAP50: {df.loc[best_seed, 'mAP50']:.4f})\")\n    \n    print(\"=\"*60)\nelse:\n    print(\"No results to display. Training may have failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:21:05.442794Z","iopub.execute_input":"2026-01-29T02:21:05.443546Z","iopub.status.idle":"2026-01-29T02:21:05.467022Z","shell.execute_reply.started":"2026-01-29T02:21:05.443502Z","shell.execute_reply":"2026-01-29T02:21:05.466056Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nA.5 LATE FUSION (V2) - FINAL RESULTS\n============================================================\n\nPer-Seed Results:\n      mAP50  mAP50-95\nSeed                 \n42   0.5974    0.1941\n123  0.7554    0.2728\n456  0.5618    0.2112\n789  0.7441    0.2648\n101  0.7122    0.2463\n\n------------------------------------------------------------\nSTATISTICAL SUMMARY\n------------------------------------------------------------\nMetric                Mean        Std        Min        Max\n------------------------------------------------------------\nmAP50               0.6742     0.0887     0.5618     0.7554\nmAP50-95            0.2378     0.0341     0.1941     0.2728\n\nBest Seed: 123 (mAP50: 0.7554)\n============================================================\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# =============================================================================\n# Cell 13: Save Results (Same Format as A.1-A.4b)\n# =============================================================================\n\noutput_file = KAGGLE_OUTPUT / 'a5_late_fusion_v2_results.txt'\n\nwith open(output_file, 'w') as f:\n    f.write(\"=\"*60 + \"\\n\")\n    f.write(\"A.5 Late Fusion (V2) Results\\n\")\n    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    f.write(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    \n    f.write(\"Configuration:\\n\")\n    f.write(f\"  Architecture: Late Fusion (RGB + Depth)\\n\")\n    f.write(f\"  RGB Backbone: Frozen (from A.1)\\n\")\n    f.write(f\"  Depth Backbone: Frozen (from A.2)\\n\")\n    f.write(f\"  Trainable: Fusion Layer + Detection Head\\n\")\n    f.write(f\"  Model: YOLOv11n (backbones)\\n\")\n    f.write(f\"  Epochs: {EPOCHS} (patience: {PATIENCE})\\n\")\n    f.write(f\"  Image Size: {IMGSZ}\\n\")\n    f.write(f\"  Batch Size: {BATCH_SIZE}\\n\")\n    f.write(f\"  Seeds: {SEEDS}\\n\")\n    \n    f.write(\"\\nUniform Augmentation:\\n\")\n    for key, value in AUGMENT_PARAMS.items():\n        f.write(f\"  {key}: {value}\\n\")\n    \n    if results_dict:\n        f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n        f.write(\"Per-Seed Results:\\n\")\n        f.write(\"=\"*60 + \"\\n\")\n        f.write(df.to_string(float_format=lambda x: f\"{x:.4f}\"))\n        \n        f.write(\"\\n\\n\" + \"-\"*60 + \"\\n\")\n        f.write(\"Summary (Mean +/- Std):\\n\")\n        f.write(\"-\"*60 + \"\\n\")\n        for col in df.columns:\n            f.write(f\"  {col}: {avg[col]:.4f} +/- {std[col]:.4f}\\n\")\n        \n        f.write(f\"\\nBest Seed: {best_seed}\\n\")\n\nprint(f\"\\nResults saved: {output_file}\")\n\n# Save as JSON (same format as other experiments)\njson_output = {\n    'experiment': 'A.5',\n    'variant': 'V2',\n    'name': 'Late Fusion',\n    'seeds': SEEDS,\n    'config': {\n        'model': 'yolo11n',\n        'architecture': 'late_fusion',\n        'rgb_backbone': 'frozen_from_a1',\n        'depth_backbone': 'frozen_from_a2',\n        'epochs': EPOCHS,\n        'patience': PATIENCE,\n        'imgsz': IMGSZ,\n        'batch': BATCH_SIZE,\n        'augmentation': AUGMENT_PARAMS,\n    },\n    'results': {str(k): v for k, v in results_dict.items()} if results_dict else {},\n    'summary': {\n        'mean': {k: float(v) for k, v in avg.items()},\n        'std': {k: float(v) for k, v in std.items()},\n        'best_seed': int(best_seed) if results_dict else None,\n    } if results_dict else None,\n}\n\njson_file = KAGGLE_OUTPUT / 'a5_late_fusion_v2_results.json'\nwith open(json_file, 'w') as f:\n    json.dump(json_output, f, indent=2)\n\nprint(f\"JSON saved: {json_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:21:10.691928Z","iopub.execute_input":"2026-01-29T02:21:10.692257Z","iopub.status.idle":"2026-01-29T02:21:10.706333Z","shell.execute_reply.started":"2026-01-29T02:21:10.692227Z","shell.execute_reply":"2026-01-29T02:21:10.705744Z"}},"outputs":[{"name":"stdout","text":"\nResults saved: /kaggle/working/kaggleoutput/a5_late_fusion_v2_results.txt\nJSON saved: /kaggle/working/kaggleoutput/a5_late_fusion_v2_results.json\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# =============================================================================\n# Cell 14: Create Archives for Download\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING ARCHIVES\")\nprint(\"=\"*60 + \"\\n\")\n\n# Archive training runs\nif RUNS_PATH.exists():\n    runs_zip = BASE_PATH / 'a5_late_fusion_v2_runs'\n    shutil.make_archive(str(runs_zip), 'zip', RUNS_PATH)\n    size_mb = (runs_zip.with_suffix('.zip')).stat().st_size / 1024 / 1024\n    print(f\"a5_late_fusion_v2_runs.zip: {size_mb:.1f} MB\")\n\n# Archive outputs\noutput_zip = BASE_PATH / 'a5_late_fusion_v2_output'\nshutil.make_archive(str(output_zip), 'zip', KAGGLE_OUTPUT)\nsize_mb = (output_zip.with_suffix('.zip')).stat().st_size / 1024 / 1024\nprint(f\"a5_late_fusion_v2_output.zip: {size_mb:.1f} MB\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALL DONE!\")\nprint(\"=\"*60)\nprint(\"\\nDownload from Output tab:\")\nprint(\"  - a5_late_fusion_v2_runs.zip (training runs)\")\nprint(\"  - a5_late_fusion_v2_output.zip (results)\")\nprint(\"\\nOutput files:\")\nprint(f\"  - {output_file}\")\nprint(f\"  - {json_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T02:21:13.834887Z","iopub.execute_input":"2026-01-29T02:21:13.835229Z","iopub.status.idle":"2026-01-29T02:22:01.218130Z","shell.execute_reply.started":"2026-01-29T02:21:13.835190Z","shell.execute_reply":"2026-01-29T02:22:01.217199Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nCREATING ARCHIVES\n============================================================\n\na5_late_fusion_v2_runs.zip: 162.5 MB\na5_late_fusion_v2_output.zip: 0.0 MB\n\n============================================================\nALL DONE!\n============================================================\n\nDownload from Output tab:\n  - a5_late_fusion_v2_runs.zip (training runs)\n  - a5_late_fusion_v2_output.zip (results)\n\nOutput files:\n  - /kaggle/working/kaggleoutput/a5_late_fusion_v2_results.txt\n  - /kaggle/working/kaggleoutput/a5_late_fusion_v2_results.json\n","output_type":"stream"}],"execution_count":19}]}