{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.2 - Two-Stage Ripeness Classification\n",
    "\n",
    "**Experiment**: B.2\n",
    "**Approach**: Detect → Crop → Classify (2 stages)\n",
    "**Objective**: Test if separating detection and classification improves accuracy\n",
    "**Comparison**: B.1 End-to-End (mAP50=0.801, mAP50-95=0.514)\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Stage 1**: Detector - Detect all FFBs (ripe/unripe) using YOLO detection\n",
    "2. **Crop**: Extract bounding boxes with 10% margin\n",
    "3. **Stage 2**: Classifier - Classify crops using YOLO classification\n",
    "4. **Inference**: End-to-end pipeline on test set\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "BASE_PATH = Path('/kaggle/working' if IS_KAGGLE else r'd:\\Work\\Assisten Dosen\\Anylabel\\Experiments')\n",
    "DATASET_PATH = Path('/kaggle/input/ffb-ripeness' if IS_KAGGLE else BASE_PATH / 'datasets' / 'ffb_ripeness')\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"Dataset: {DATASET_PATH}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Train Detector (Ripe/Unripe Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 Config\n",
    "stage1_config_path = BASE_PATH / 'configs' / 'ffb_ripeness_detect.yaml'\n",
    "stage1_config_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "yaml_content = f\"\"\"path: {DATASET_PATH.as_posix()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "nc: 2\n",
    "names: ['ripe', 'unripe']\n",
    "\"\"\"\n",
    "stage1_config_path.write_text(yaml_content)\n",
    "print(f\"Stage 1 config: {stage1_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stage 1 (2 seeds)\n",
    "STAGE1_CONFIG = {\n",
    "    'model': 'yolo11n.pt',\n",
    "    'epochs': 50,\n",
    "    'batch': 16,\n",
    "    'imgsz': 640,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "SEEDS = [42, 123]\n",
    "STAGE1_PREFIX = 'exp_b2_stage1'\n",
    "\n",
    "stage1_results = {}\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n{'='*60}\\nSTAGE 1 - DETECTOR - SEED {seed}\\n{'='*60}\\n\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = YOLO(STAGE1_CONFIG['model'])\n",
    "    results = model.train(\n",
    "        data=str(stage1_config_path),\n",
    "        epochs=STAGE1_CONFIG['epochs'],\n",
    "        batch=STAGE1_CONFIG['batch'],\n",
    "        imgsz=STAGE1_CONFIG['imgsz'],\n",
    "        device=STAGE1_CONFIG['device'],\n",
    "        name=f\"{STAGE1_PREFIX}_seed{seed}\",\n",
    "        seed=seed,\n",
    "        exist_ok=True\n",
    "    )\n",
    "    stage1_results[seed] = str(BASE_PATH / 'runs' / 'detect' / f\"{STAGE1_PREFIX}_seed{seed}\" / 'weights' / 'best.pt')\n",
    "    print(f\"✅ Stage 1 seed {seed} complete\")\n",
    "\n",
    "print(f\"\\nStage 1 models saved:\")\n",
    "for seed, path in stage1_results.items():\n",
    "    print(f\"  Seed {seed}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Extract Crops with 10% Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract crops function\n",
    "def extract_crops_with_margin(detector_model_path, image_dir, label_dir, output_dir, margin=0.1):\n",
    "    \"\"\"Extract crops from detections with margin\"\"\"\n",
    "    model = YOLO(detector_model_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create output dirs\n",
    "    for class_name in ['ripe', 'unripe']:\n",
    "        (output_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_files = list(Path(image_dir).glob('*.png')) + list(Path(image_dir).glob('*.jpg'))\n",
    "    stats = {'ripe': 0, 'unripe': 0}\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Extracting crops\"):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Get GT labels\n",
    "        label_path = Path(label_dir) / f\"{img_path.stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, width, height = map(float, parts[1:5])\n",
    "            \n",
    "            # Convert to pixel coords\n",
    "            x_center *= w\n",
    "            y_center *= h\n",
    "            width *= w\n",
    "            height *= h\n",
    "            \n",
    "            # Add margin\n",
    "            width_margin = width * margin\n",
    "            height_margin = height * margin\n",
    "            x1 = int(max(0, x_center - (width + width_margin) / 2))\n",
    "            y1 = int(max(0, y_center - (height + height_margin) / 2))\n",
    "            x2 = int(min(w, x_center + (width + width_margin) / 2))\n",
    "            y2 = int(min(h, y_center + (height + height_margin) / 2))\n",
    "            \n",
    "            # Crop\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "            \n",
    "            # Save\n",
    "            class_name = 'ripe' if class_id == 0 else 'unripe'\n",
    "            crop_filename = f\"{img_path.stem}_crop{i}.png\"\n",
    "            crop_path = output_dir / class_name / crop_filename\n",
    "            cv2.imwrite(str(crop_path), crop)\n",
    "            stats[class_name] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Crop extraction function defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract crops for all splits (using best detector from stage 1)\n",
    "CROPS_BASE = BASE_PATH / 'datasets' / 'ffb_ripeness_twostage_crops'\n",
    "best_detector = stage1_results[42]  # Use seed 42 detector\n",
    "\n",
    "print(f\"Extracting crops using: {best_detector}\\n\")\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{'='*60}\\nExtracting {split.upper()} crops\\n{'='*60}\\n\")\n",
    "    \n",
    "    img_dir = DATASET_PATH / 'images' / split\n",
    "    label_dir = DATASET_PATH / 'labels' / split\n",
    "    output_dir = CROPS_BASE / split\n",
    "    \n",
    "    stats = extract_crops_with_margin(best_detector, img_dir, label_dir, output_dir, margin=0.1)\n",
    "    \n",
    "    print(f\"\\n{split.upper()} crops extracted:\")\n",
    "    print(f\"  Ripe: {stats['ripe']}\")\n",
    "    print(f\"  Unripe: {stats['unripe']}\")\n",
    "    print(f\"  Total: {sum(stats.values())}\")\n",
    "\n",
    "print(f\"\\n✅ All crops saved to: {CROPS_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Train Classifier on Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 Config (Classification)\n",
    "stage2_config_path = BASE_PATH / 'configs' / 'ffb_ripeness_classify.yaml'\n",
    "\n",
    "yaml_content = f\"\"\"path: {CROPS_BASE.as_posix()}\n",
    "train: train\n",
    "val: val\n",
    "test: test\n",
    "nc: 2\n",
    "names: ['ripe', 'unripe']\n",
    "\"\"\"\n",
    "stage2_config_path.write_text(yaml_content)\n",
    "print(f\"Stage 2 config: {stage2_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stage 2 Classifier (2 seeds)\n",
    "STAGE2_CONFIG = {\n",
    "    'model': 'yolo11n-cls.pt',\n",
    "    'epochs': 100,\n",
    "    'batch': 32,\n",
    "    'imgsz': 224,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "STAGE2_PREFIX = 'exp_b2_stage2'\n",
    "\n",
    "stage2_results = {}\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n{'='*60}\\nSTAGE 2 - CLASSIFIER - SEED {seed}\\n{'='*60}\\n\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = YOLO(STAGE2_CONFIG['model'])\n",
    "    results = model.train(\n",
    "        data=str(stage2_config_path),\n",
    "        epochs=STAGE2_CONFIG['epochs'],\n",
    "        batch=STAGE2_CONFIG['batch'],\n",
    "        imgsz=STAGE2_CONFIG['imgsz'],\n",
    "        device=STAGE2_CONFIG['device'],\n",
    "        name=f\"{STAGE2_PREFIX}_seed{seed}\",\n",
    "        seed=seed,\n",
    "        exist_ok=True\n",
    "    )\n",
    "    stage2_results[seed] = str(BASE_PATH / 'runs' / 'classify' / f\"{STAGE2_PREFIX}_seed{seed}\" / 'weights' / 'best.pt')\n",
    "    print(f\"✅ Stage 2 seed {seed} complete\")\n",
    "\n",
    "print(f\"\\nStage 2 models saved:\")\n",
    "for seed, path in stage2_results.items():\n",
    "    print(f\"  Seed {seed}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: End-to-End Inference & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end inference function\n",
    "def two_stage_inference(detector_path, classifier_path, test_images_dir, test_labels_dir):\n",
    "    \"\"\"Run two-stage pipeline: detect -> classify\"\"\"\n",
    "    detector = YOLO(detector_path)\n",
    "    classifier = YOLO(classifier_path)\n",
    "    \n",
    "    image_files = list(Path(test_images_dir).glob('*.png')) + list(Path(test_images_dir).glob('*.jpg'))\n",
    "    \n",
    "    results = {'correct': 0, 'wrong': 0, 'missed': 0}\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Two-stage inference\"):\n",
    "        # Stage 1: Detect\n",
    "        detections = detector(str(img_path), verbose=False)\n",
    "        \n",
    "        # Stage 2: Classify each detection\n",
    "        img = cv2.imread(str(img_path))\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Get GT labels\n",
    "        label_path = Path(test_labels_dir) / f\"{img_path.stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            gt_labels = [int(line.split()[0]) for line in f.readlines()]\n",
    "        \n",
    "        # Process detections\n",
    "        for det in detections[0].boxes:\n",
    "            x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            \n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "            \n",
    "            # Classify crop\n",
    "            cls_result = classifier(crop, verbose=False)\n",
    "            pred_class = int(cls_result[0].probs.top1)\n",
    "            \n",
    "            # Compare with GT (simplified - assumes 1:1 mapping)\n",
    "            if gt_labels:\n",
    "                gt_class = gt_labels.pop(0)\n",
    "                if pred_class == gt_class:\n",
    "                    results['correct'] += 1\n",
    "                else:\n",
    "                    results['wrong'] += 1\n",
    "        \n",
    "        results['missed'] += len(gt_labels)  # Undetected objects\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Inference function defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run end-to-end evaluation for both seeds\n",
    "e2e_results = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n{'='*60}\\nEND-TO-END EVALUATION - SEED {seed}\\n{'='*60}\\n\")\n",
    "    \n",
    "    results = two_stage_inference(\n",
    "        stage1_results[seed],\n",
    "        stage2_results[seed],\n",
    "        DATASET_PATH / 'images' / 'test',\n",
    "        DATASET_PATH / 'labels' / 'test'\n",
    "    )\n",
    "    \n",
    "    total = results['correct'] + results['wrong']\n",
    "    accuracy = results['correct'] / total if total > 0 else 0\n",
    "    \n",
    "    e2e_results[seed] = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': results['correct'],\n",
    "        'wrong': results['wrong'],\n",
    "        'missed': results['missed']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults for seed {seed}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Correct: {results['correct']}\")\n",
    "    print(f\"  Wrong: {results['wrong']}\")\n",
    "    print(f\"  Missed: {results['missed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Comparison with B.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "df = pd.DataFrame(e2e_results).T\n",
    "avg = df.mean()\n",
    "avg.name = 'Average'\n",
    "df = pd.concat([df, avg.to_frame().T])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"B.2 TWO-STAGE CLASSIFICATION - RESULTS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(df.to_string(float_format=lambda x: f\"{x:.3f}\" if isinstance(x, float) else f\"{x}\"))\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: B.1 END-TO-END vs B.2 TWO-STAGE\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(\"B.1 (End-to-End): mAP50=0.801, mAP50-95=0.514\")\n",
    "print(f\"B.2 (Two-Stage): Accuracy={avg['accuracy']:.3f}\")\n",
    "print(\"\\nNote: Different metrics - B.1 uses mAP, B.2 uses classification accuracy\")\n",
    "\n",
    "# Save\n",
    "output_file = BASE_PATH / 'kaggleoutput' / 'test_twostage.txt'\n",
    "output_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(f\"B.2 Two-Stage Classification Results\\nGenerated: {datetime.now()}\\n\\n\")\n",
    "    f.write(df.to_string())\n",
    "print(f\"\\n✅ Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Two-stage pipeline:\n",
    "1. ✅ Stage 1 (Detector): Trained to detect ripe/unripe FFBs\n",
    "2. ✅ Crops extracted with 10% margin\n",
    "3. ✅ Stage 2 (Classifier): Trained on crops\n",
    "4. ✅ End-to-end evaluation completed\n",
    "\n",
    "**Key Insight**: Two-stage approach allows:\n",
    "- Specialization of detector and classifier\n",
    "- Cropping focuses classifier on relevant region\n",
    "- Potentially better accuracy than end-to-end\n",
    "\n",
    "**Trade-offs**:\n",
    "- More complex pipeline (2 models)\n",
    "- Slower inference\n",
    "- Better interpretability (can debug each stage separately)\n",
    "\n",
    "Compare results with B.1 to determine best approach for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
