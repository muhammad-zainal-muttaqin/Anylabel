"""
Prepare Synthetic Depth Dataset untuk A.4a (Depth-Only)

Organize synthetic depth maps into proper YOLO dataset structure,
similar to prepare_depth_data.py for real depth.

Usage:
    python prepare_synthetic_depth_data.py

Requirements:
    - Synthetic depth maps generated (run generate_synthetic_depth.py first)
    - RGB labels from ffb_localization dataset

Author: Research Team
Date: 2026-01-21
"""

import shutil
from pathlib import Path

PROJECT_DIR = Path(__file__).resolve().parents[2]
DATASET_DIR = PROJECT_DIR / "Experiments" / "datasets"

SYNTHETIC_DEPTH_SOURCE = DATASET_DIR / "depth_synthetic_da2"
OUTPUT_DIR = DATASET_DIR / "ffb_localization_depth_synthetic"


def prepare_dataset():
    """
    Copy synthetic depth images and labels to proper YOLO structure.
    """
    print("Preparing synthetic depth dataset...")
    print(f"Source: {SYNTHETIC_DEPTH_SOURCE}")
    print(f"Output: {OUTPUT_DIR}")

    # Create directory structure
    for split in ["train", "val", "test"]:
        (OUTPUT_DIR / "images" / split).mkdir(parents=True, exist_ok=True)
        (OUTPUT_DIR / "labels" / split).mkdir(parents=True, exist_ok=True)

    # Copy depth images and labels for each split
    total_images = 0
    total_labels = 0

    for split in ["train", "val", "test"]:
        source_depth = SYNTHETIC_DEPTH_SOURCE / split
        source_labels = DATASET_DIR / "ffb_localization" / "labels" / split

        dest_images = OUTPUT_DIR / "images" / split
        dest_labels = OUTPUT_DIR / "labels" / split

        if not source_depth.exists():
            print(f"  Warning: {split} depth not found at {source_depth}")
            continue

        if not source_labels.exists():
            print(f"  Warning: {split} labels not found at {source_labels}")
            continue

        # Copy depth images
        depth_count = 0
        for depth_file in source_depth.glob("*.png"):
            shutil.copy2(depth_file, dest_images / depth_file.name)
            depth_count += 1

        # Copy labels
        label_count = 0
        for label_file in source_labels.glob("*.txt"):
            shutil.copy2(label_file, dest_labels / label_file.name)
            label_count += 1

        print(f"  {split}: {depth_count} images, {label_count} labels")
        total_images += depth_count
        total_labels += label_count

    print(f"\nTotal: {total_images} images, {total_labels} labels")
    print("Dataset ready!")

    return total_images, total_labels


def create_yaml():
    """
    Create YAML config for synthetic depth dataset.
    """
    yaml_content = f"""# Dataset Configuration for FFB Localization - SYNTHETIC DEPTH
# Generated by prepare_synthetic_depth_data.py

path: {str(OUTPUT_DIR).replace(chr(92), "/")}
train: images/train
val: images/val
test: images/test

nc: 1
names: ['fresh_fruit_bunch']

# Note: This dataset uses synthetic depth generated by Depth-Anything-V2
# For comparison with real depth, see ffb_localization_depth.yaml
"""

    yaml_path = (
        PROJECT_DIR
        / "Experiments"
        / "configs"
        / "ffb_localization_depth_synthetic.yaml"
    )
    yaml_path.write_text(yaml_content)
    print(f"\nConfig created: {yaml_path}")

    return yaml_path


def verify_dataset():
    """
    Verify dataset integrity: check if images and labels match.
    """
    print("\nVerifying dataset integrity...")

    issues = []

    for split in ["train", "val", "test"]:
        images_dir = OUTPUT_DIR / "images" / split
        labels_dir = OUTPUT_DIR / "labels" / split

        if not images_dir.exists() or not labels_dir.exists():
            continue

        # Get filenames without extension
        image_stems = {f.stem for f in images_dir.glob("*.png")}
        label_stems = {f.stem for f in labels_dir.glob("*.txt")}

        # Check for mismatches
        missing_labels = image_stems - label_stems
        missing_images = label_stems - image_stems

        if missing_labels:
            issues.append(f"{split}: {len(missing_labels)} images without labels")

        if missing_images:
            issues.append(f"{split}: {len(missing_images)} labels without images")

        print(
            f"  {split}: {len(image_stems)} images, {len(label_stems)} labels", end=""
        )
        if missing_labels or missing_images:
            print(" ⚠️ MISMATCH")
        else:
            print(" ✓")

    if issues:
        print("\n⚠️  Issues found:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print("\n✓ Dataset integrity verified!")
        return True


def main():
    print("=" * 60)
    print("Prepare Synthetic Depth Dataset for A.4a")
    print("=" * 60)

    # Check if source exists
    if not SYNTHETIC_DEPTH_SOURCE.exists():
        print(f"\n✗ Synthetic depth not found: {SYNTHETIC_DEPTH_SOURCE}")
        print("\nPlease run generate_synthetic_depth.py first:")
        print("  python generate_synthetic_depth.py")
        return

    print(f"\n✓ Synthetic depth found: {SYNTHETIC_DEPTH_SOURCE}")

    # Prepare dataset
    print("\n" + "-" * 60)
    total_images, total_labels = prepare_dataset()

    if total_images == 0:
        print("\n✗ No images copied. Check source directory.")
        return

    # Create YAML config
    print("\n" + "-" * 60)
    yaml_path = create_yaml()

    # Verify dataset
    print("\n" + "-" * 60)
    is_valid = verify_dataset()

    # Summary
    print("\n" + "=" * 60)
    print("Synthetic Depth Dataset Preparation Complete!")
    print("=" * 60)
    print(f"\nDataset location: {OUTPUT_DIR}")
    print(f"Config file: {yaml_path}")
    print(f"Total images: {total_images}")
    print(f"Total labels: {total_labels}")
    print(f"Status: {'✓ Valid' if is_valid else '⚠️  Check warnings above'}")

    print("\nNext steps:")
    print(
        "  1. python train_a4a_synthetic_depth.py  # Train depth-only with synthetic depth"
    )
    print("  2. Compare A.2 (real depth) vs A.4a (synthetic depth)")


if __name__ == "__main__":
    main()
