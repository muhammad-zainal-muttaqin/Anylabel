{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2026-01-19T01:27:39.938933Z",
          "iopub.status.busy": "2026-01-19T01:27:39.938694Z",
          "iopub.status.idle": "2026-01-19T01:27:53.608460Z",
          "shell.execute_reply": "2026-01-19T01:27:53.607559Z",
          "shell.execute_reply.started": "2026-01-19T01:27:39.938911Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# Install Ultralytics\n",
        "!pip -q install ultralytics\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "try:\n",
        "    from ultralytics.models.yolo.detect import DetectionTrainer, DetectionValidator\n",
        "except Exception:\n",
        "    from ultralytics.engine.trainer import DetectionTrainer\n",
        "    from ultralytics.engine.validator import DetectionValidator\n",
        "\n",
        "from ultralytics.data.dataset import YOLODataset\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:36:44.108531Z",
          "iopub.status.busy": "2026-01-19T01:36:44.107420Z",
          "iopub.status.idle": "2026-01-19T01:36:44.127266Z",
          "shell.execute_reply": "2026-01-19T01:36:44.126553Z",
          "shell.execute_reply.started": "2026-01-19T01:36:44.108483Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset dir: /kaggle/working/ffb_localization_rgbd\n"
          ]
        }
      ],
      "source": [
        "# Paths (edit to match your Kaggle dataset name)\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "DATASET_ZIP = Path(\"/kaggle/input/ffb-localization-rgbd/ffb_localization_rgbd.zip\")\n",
        "DATASET_DIR = Path(\"/kaggle/input/ffb-localization-rgbd-dataset/ffb_localization_rgbd\")\n",
        "\n",
        "WORK_DIR = Path(\"/kaggle/working/ffb_localization_rgbd\")\n",
        "SYNC_TO_WORKDIR = True  # copy to /kaggle/working to allow sync/delete\n",
        "\n",
        "\n",
        "def find_dataset_root(base: Path) -> Path:\n",
        "    candidates = [base, base / \"ffb_localization_rgbd\"]\n",
        "    for c in candidates:\n",
        "        if (c / \"rgb\").exists() and (c / \"depth\").exists() and (c / \"labels\").exists():\n",
        "            return c\n",
        "    if base.exists():\n",
        "        for p in base.iterdir():\n",
        "            if p.is_dir() and (p / \"rgb\").exists() and (p / \"depth\").exists() and (p / \"labels\").exists():\n",
        "                return p\n",
        "    raise FileNotFoundError(f\"RGBD dataset root not found under: {base}\")\n",
        "\n",
        "\n",
        "def find_zip(base: Path) -> Path | None:\n",
        "    if not base.exists():\n",
        "        return None\n",
        "    zips = list(base.glob(\"*.zip\"))\n",
        "    if len(zips) == 1:\n",
        "        return zips[0]\n",
        "    return None\n",
        "\n",
        "\n",
        "if not DATASET_ZIP.exists():\n",
        "    auto_zip = find_zip(DATASET_DIR) or find_zip(DATASET_DIR.parent)\n",
        "    if auto_zip:\n",
        "        DATASET_ZIP = auto_zip\n",
        "        print(\"Auto ZIP:\", DATASET_ZIP)\n",
        "\n",
        "if DATASET_ZIP.exists():\n",
        "    if not WORK_DIR.exists():\n",
        "        shutil.unpack_archive(str(DATASET_ZIP), str(WORK_DIR))\n",
        "    DATASET_DIR = WORK_DIR\n",
        "elif SYNC_TO_WORKDIR and DATASET_DIR.exists() and DATASET_DIR.as_posix().startswith(\"/kaggle/input\"):\n",
        "    if not WORK_DIR.exists():\n",
        "        shutil.copytree(DATASET_DIR, WORK_DIR)\n",
        "    DATASET_DIR = WORK_DIR\n",
        "\n",
        "DATASET_DIR = find_dataset_root(DATASET_DIR)\n",
        "\n",
        "print(\"Dataset dir:\", DATASET_DIR)\n",
        "for sub in (\"rgb\", \"depth\", \"labels\"):\n",
        "    assert (DATASET_DIR / sub).exists(), f\"Missing {sub} folder in {DATASET_DIR}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:36:47.286143Z",
          "iopub.status.busy": "2026-01-19T01:36:47.285376Z",
          "iopub.status.idle": "2026-01-19T01:36:47.291847Z",
          "shell.execute_reply": "2026-01-19T01:36:47.291033Z",
          "shell.execute_reply.started": "2026-01-19T01:36:47.286110Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /kaggle/working/ffb_localization_rgbd_train.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /kaggle/working/ffb_localization_rgbd_train.yaml\n",
        "# Use /kaggle/working (writable) to allow cache files\n",
        "# IMPORTANT: use standard 'images/' + 'labels/' structure so Ultralytics can discover labels.\n",
        "path: /kaggle/working/ffb_localization_rgbd\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "nc: 1\n",
        "names: ['fresh_fruit_bunch']\n",
        "channels: 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:36:49.062756Z",
          "iopub.status.busy": "2026-01-19T01:36:49.062387Z",
          "iopub.status.idle": "2026-01-19T01:36:49.077148Z",
          "shell.execute_reply": "2026-01-19T01:36:49.076268Z",
          "shell.execute_reply.started": "2026-01-19T01:36:49.062696Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: keep=280 drop_rgb=0 drop_depth=0 drop_labels=0\n",
            "val: keep=80 drop_rgb=0 drop_depth=0 drop_labels=0\n",
            "test: keep=40 drop_rgb=0 drop_depth=0 drop_labels=0\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if \"DATASET_DIR\" not in globals():\n",
        "    raise RuntimeError(\"Run the Paths cell first.\")\n",
        "\n",
        "\n",
        "def sync_rgbd_dataset(root: Path) -> None:\n",
        "    splits = (\"train\", \"val\", \"test\")\n",
        "    for split in splits:\n",
        "        rgb_dir = root / \"rgb\" / split\n",
        "        depth_dir = root / \"depth\" / split\n",
        "        label_dir = root / \"labels\" / split\n",
        "\n",
        "        rgb_files = {p.name for p in rgb_dir.glob(\"*.png\")}\n",
        "        depth_files = {p.name for p in depth_dir.glob(\"*.png\")}\n",
        "        label_files = {p.with_suffix(\".png\").name for p in label_dir.glob(\"*.txt\")}\n",
        "\n",
        "        keep = rgb_files & depth_files & label_files\n",
        "        drop_rgb = rgb_files - keep\n",
        "        drop_depth = depth_files - keep\n",
        "        drop_labels = {f.replace(\".png\", \".txt\") for f in (label_files - keep)}\n",
        "\n",
        "        for f in drop_rgb:\n",
        "            (rgb_dir / f).unlink(missing_ok=True)\n",
        "        for f in drop_depth:\n",
        "            (depth_dir / f).unlink(missing_ok=True)\n",
        "        for f in drop_labels:\n",
        "            (label_dir / f).unlink(missing_ok=True)\n",
        "\n",
        "        print(\n",
        "            f\"{split}: keep={len(keep)} drop_rgb={len(drop_rgb)} \"\n",
        "            f\"drop_depth={len(drop_depth)} drop_labels={len(drop_labels)}\"\n",
        "        )\n",
        "\n",
        "\n",
        "sync_rgbd_dataset(DATASET_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build standard Ultralytics structure + sanity checks\n",
        "# - Ultralytics expects train/val/test under 'images/' and labels under 'labels/' (same split)\n",
        "# - We keep your existing 'rgb/' + 'depth/' folders, but expose RGB also via 'images/'\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "root = Path(\"/kaggle/working/ffb_localization_rgbd\")\n",
        "if \"DATASET_DIR\" in globals():\n",
        "    root = Path(str(DATASET_DIR))\n",
        "\n",
        "# 1) Create /images/{split} as a copy (or refresh) of /rgb/{split}\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    src = root / \"rgb\" / split\n",
        "    dst = root / \"images\" / split\n",
        "    dst.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    src_files = sorted(src.glob(\"*.png\"))\n",
        "    for p in src_files:\n",
        "        out = dst / p.name\n",
        "        if not out.exists():\n",
        "            shutil.copy2(p, out)\n",
        "\n",
        "    print(f\"{split}: images={len(list(dst.glob('*.png')))} (from rgb={len(src_files)})\")\n",
        "\n",
        "# 2) Verify labels are non-empty (otherwise everything becomes background)\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    label_dir = root / \"labels\" / split\n",
        "    lbls = sorted(label_dir.glob(\"*.txt\"))\n",
        "    nonempty = [p for p in lbls if p.read_text(encoding=\"utf-8\").strip()]\n",
        "    print(f\"{split}: labels={len(lbls)} nonempty={len(nonempty)}\")\n",
        "    if not nonempty:\n",
        "        raise RuntimeError(\n",
        "            f\"No non-empty labels in {label_dir}. \"\n",
        "            \"Fix dataset: YOLO .txt must contain lines: <cls> <x> <y> <w> <h>.\"\n",
        "        )\n",
        "\n",
        "sample = nonempty[0]\n",
        "print(\"Sample label:\", sample.name)\n",
        "print(\"\\n\".join(sample.read_text(encoding=\"utf-8\").splitlines()[:3]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:36:51.349911Z",
          "iopub.status.busy": "2026-01-19T01:36:51.349277Z",
          "iopub.status.idle": "2026-01-19T01:36:51.356998Z",
          "shell.execute_reply": "2026-01-19T01:36:51.356214Z",
          "shell.execute_reply.started": "2026-01-19T01:36:51.349878Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache cleared.\n"
          ]
        }
      ],
      "source": [
        "# Remove old caches (important after changing image/label mapping)\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/kaggle/working/ffb_localization_rgbd\")\n",
        "if \"DATASET_DIR\" in globals():\n",
        "    root = Path(str(DATASET_DIR))\n",
        "\n",
        "for p in root.rglob(\"*.cache\"):\n",
        "    try:\n",
        "        p.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"Cache cleared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:36:58.694915Z",
          "iopub.status.busy": "2026-01-19T01:36:58.694572Z",
          "iopub.status.idle": "2026-01-19T01:36:58.703956Z",
          "shell.execute_reply": "2026-01-19T01:36:58.703235Z",
          "shell.execute_reply.started": "2026-01-19T01:36:58.694883Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def normalize_depth_to_uint8(depth: np.ndarray) -> np.ndarray:\n",
        "    if depth.dtype == np.uint8:\n",
        "        return depth\n",
        "    depth_f = depth.astype(np.float32)\n",
        "    norm = cv2.normalize(depth_f, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    return norm.astype(np.uint8)\n",
        "\n",
        "\n",
        "class RGBDDataset(YOLODataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Ultralytics Mosaic may sample from dataset.buffer; in some setups this buffer starts empty\n",
        "        # and causes: IndexError in random.choices(list(self.dataset.buffer), ...)\n",
        "        try:\n",
        "            from collections import deque\n",
        "\n",
        "            buf = getattr(self, \"buffer\", None)\n",
        "            if buf is None or len(buf) == 0:\n",
        "                n = len(getattr(self, \"im_files\", []))\n",
        "                seed = list(range(min(n, 256)))  # small warm buffer is enough\n",
        "                self.buffer = deque(seed, maxlen=1000)\n",
        "        except Exception:\n",
        "            # If anything goes wrong, continue without buffer init.\n",
        "            pass\n",
        "\n",
        "    @staticmethod\n",
        "    def img2label_paths(img_paths):\n",
        "        # Keep robust: support both /images/ (standard) and /rgb/ (legacy)\n",
        "        label_paths = []\n",
        "        for p in img_paths:\n",
        "            p = str(p)\n",
        "            if (os.sep + \"images\" + os.sep) in p:\n",
        "                p = p.replace(os.sep + \"images\" + os.sep, os.sep + \"labels\" + os.sep)\n",
        "            elif (os.sep + \"rgb\" + os.sep) in p:\n",
        "                p = p.replace(os.sep + \"rgb\" + os.sep, os.sep + \"labels\" + os.sep)\n",
        "            label_paths.append(os.path.splitext(p)[0] + \".txt\")\n",
        "        return label_paths\n",
        "\n",
        "    def load_image(self, i):\n",
        "        f = self.im_files[i]\n",
        "        rgb = cv2.imread(f)\n",
        "        if rgb is None:\n",
        "            raise FileNotFoundError(f\"RGB not found: {f}\")\n",
        "        h0, w0 = rgb.shape[:2]\n",
        "\n",
        "        # Depth path mapping: /images/ -> /depth/ (preferred), fallback /rgb/ -> /depth/\n",
        "        if (os.sep + \"images\" + os.sep) in f:\n",
        "            depth_path = f.replace(os.sep + \"images\" + os.sep, os.sep + \"depth\" + os.sep)\n",
        "        else:\n",
        "            depth_path = f.replace(os.sep + \"rgb\" + os.sep, os.sep + \"depth\" + os.sep)\n",
        "\n",
        "        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "        if depth is None:\n",
        "            raise FileNotFoundError(f\"Depth not found: {depth_path}\")\n",
        "        if depth.ndim == 3:\n",
        "            depth = depth[:, :, 0]\n",
        "        if depth.shape[:2] != (h0, w0):\n",
        "            depth = cv2.resize(depth, (w0, h0), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        depth = normalize_depth_to_uint8(depth)[:, :, None]\n",
        "        img = np.concatenate([rgb, depth], axis=2)\n",
        "\n",
        "        # Resize like YOLODataset\n",
        "        r = self.imgsz / max(h0, w0)\n",
        "        if r != 1:\n",
        "            interp = cv2.INTER_LINEAR if (self.augment or r > 1) else cv2.INTER_AREA\n",
        "            img = cv2.resize(img, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
        "\n",
        "        return img, (h0, w0), img.shape[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:37:02.758764Z",
          "iopub.status.busy": "2026-01-19T01:37:02.758421Z",
          "iopub.status.idle": "2026-01-19T01:37:02.766439Z",
          "shell.execute_reply": "2026-01-19T01:37:02.765626Z",
          "shell.execute_reply.started": "2026-01-19T01:37:02.758698Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class RGBDTrainer(DetectionTrainer):\n",
        "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
        "        # Trainer has self.model\n",
        "        stride_t = self.model.stride\n",
        "        stride = int(stride_t.max()) if hasattr(stride_t, \"max\") else int(stride_t)\n",
        "        return RGBDDataset(\n",
        "            data=self.data,\n",
        "            task=self.args.task,\n",
        "            img_path=img_path,\n",
        "            imgsz=self.args.imgsz,\n",
        "            batch_size=batch,\n",
        "            augment=mode == \"train\",\n",
        "            hyp=self.args,\n",
        "            rect=mode == \"val\",\n",
        "            cache=self.args.cache,\n",
        "            single_cls=False,\n",
        "            stride=stride,\n",
        "            pad=0.0,\n",
        "            prefix=f\"{mode}: \",\n",
        "        )\n",
        "\n",
        "\n",
        "class RGBDValidator(DetectionValidator):\n",
        "    def build_dataset(self, img_path, mode=\"val\", batch=None):\n",
        "        # IMPORTANT: Validator does NOT have self.model at build_dataset time.\n",
        "        # Ultralytics sets self.stride in validator.__call__(..., model) before get_dataloader().\n",
        "        stride_t = getattr(self, \"stride\", 32)\n",
        "        stride = int(stride_t.max()) if hasattr(stride_t, \"max\") else int(stride_t)\n",
        "        return RGBDDataset(\n",
        "            data=self.data,\n",
        "            task=self.args.task,\n",
        "            img_path=img_path,\n",
        "            imgsz=self.args.imgsz,\n",
        "            batch_size=batch,\n",
        "            augment=False,\n",
        "            hyp=self.args,\n",
        "            rect=True,\n",
        "            cache=self.args.cache,\n",
        "            single_cls=False,\n",
        "            stride=stride,\n",
        "            pad=0.0,\n",
        "            prefix=f\"{mode}: \",\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:37:05.158751Z",
          "iopub.status.busy": "2026-01-19T01:37:05.157959Z",
          "iopub.status.idle": "2026-01-19T01:37:05.167198Z",
          "shell.execute_reply": "2026-01-19T01:37:05.166277Z",
          "shell.execute_reply.started": "2026-01-19T01:37:05.158684Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def adapt_first_conv_to_4ch(det_model):\n",
        "    first = det_model.model[0]\n",
        "    conv = first.conv if hasattr(first, \"conv\") else first\n",
        "\n",
        "    if conv.in_channels == 4:\n",
        "        return\n",
        "\n",
        "    new_conv = torch.nn.Conv2d(\n",
        "        in_channels=4,\n",
        "        out_channels=conv.out_channels,\n",
        "        kernel_size=conv.kernel_size,\n",
        "        stride=conv.stride,\n",
        "        padding=conv.padding,\n",
        "        bias=(conv.bias is not None),\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        new_conv.weight[:, : conv.in_channels] = conv.weight\n",
        "        # init the 4th channel as mean of RGB weights\n",
        "        new_conv.weight[:, conv.in_channels :] = conv.weight.mean(dim=1, keepdim=True)\n",
        "        if conv.bias is not None:\n",
        "            new_conv.bias[:] = conv.bias\n",
        "\n",
        "    if hasattr(first, \"conv\"):\n",
        "        first.conv = new_conv\n",
        "    else:\n",
        "        det_model.model[0] = new_conv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:37:07.783942Z",
          "iopub.status.busy": "2026-01-19T01:37:07.782991Z",
          "iopub.status.idle": "2026-01-19T01:37:07.802635Z",
          "shell.execute_reply": "2026-01-19T01:37:07.801775Z",
          "shell.execute_reply.started": "2026-01-19T01:37:07.783892Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 280\n",
            "Val images: 80\n",
            "Test images: 40\n"
          ]
        }
      ],
      "source": [
        "DATA = \"/kaggle/working/ffb_localization_rgbd_train.yaml\"\n",
        "\n",
        "DATASET_ROOT = Path(\"/kaggle/input/ffb-localization-rgbd-dataset/ffb_localization_rgbd\")\n",
        "if not DATASET_ROOT.exists():\n",
        "    DATASET_ROOT = DATASET_DIR\n",
        "\n",
        "# after the build step, Ultralytics reads from /images/*\n",
        "train_dir = DATASET_ROOT / \"images\" / \"train\"\n",
        "val_dir = DATASET_ROOT / \"images\" / \"val\"\n",
        "test_dir = DATASET_ROOT / \"images\" / \"test\"\n",
        "\n",
        "print(\"Train images:\", len(list(train_dir.glob(\"*.png\"))))\n",
        "print(\"Val images:\", len(list(val_dir.glob(\"*.png\"))))\n",
        "print(\"Test images:\", len(list(test_dir.glob(\"*.png\"))))\n",
        "\n",
        "IMGSZ = 640\n",
        "EPOCHS = 100\n",
        "BATCH = 16\n",
        "DEVICE = \"0\"  # set \"cpu\" if no GPU\n",
        "\n",
        "RUNS_DIR = Path(\"/kaggle/working/runs/detect\")\n",
        "\n",
        "\n",
        "def zip_dir(dir_path: Path, zip_path: Path) -> Path:\n",
        "    zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if zip_path.suffix.lower() == \".zip\":\n",
        "        zip_path = zip_path.with_suffix(\"\")\n",
        "    out = shutil.make_archive(str(zip_path), \"zip\", root_dir=str(dir_path))\n",
        "    return Path(out)\n",
        "\n",
        "\n",
        "def run_seed(seed: int):\n",
        "    exp_name = f\"exp_a3_rgbd_seed{seed}\"\n",
        "\n",
        "    model = YOLO(\"yolo11n.pt\")\n",
        "    adapt_first_conv_to_4ch(model.model)\n",
        "    model.model.yaml[\"ch\"] = 4\n",
        "\n",
        "    # Train\n",
        "    model.train(\n",
        "        data=DATA,\n",
        "        imgsz=IMGSZ,\n",
        "        epochs=EPOCHS,\n",
        "        batch=BATCH,\n",
        "        seed=seed,\n",
        "        device=DEVICE,\n",
        "        name=exp_name,\n",
        "        exist_ok=True,\n",
        "        hsv_h=0.0,\n",
        "        hsv_s=0.0,\n",
        "        hsv_v=0.0,\n",
        "        trainer=RGBDTrainer,\n",
        "    )\n",
        "\n",
        "    # Test evaluation (split=test)\n",
        "    metrics = model.val(\n",
        "        data=DATA,\n",
        "        split=\"test\",\n",
        "        device=DEVICE,\n",
        "        name=f\"test_{exp_name}\",\n",
        "        exist_ok=True,\n",
        "        validator=RGBDValidator,\n",
        "    )\n",
        "\n",
        "    # Zip outputs for easy download\n",
        "    train_run_dir = RUNS_DIR / exp_name\n",
        "    test_run_dir = RUNS_DIR / f\"test_{exp_name}\"\n",
        "\n",
        "    z_train = zip_dir(train_run_dir, Path(f\"/kaggle/working/{exp_name}_train.zip\"))\n",
        "    z_test = zip_dir(test_run_dir, Path(f\"/kaggle/working/{exp_name}_test.zip\"))\n",
        "\n",
        "    print(\"Zipped:\", z_train)\n",
        "    print(\"Zipped:\", z_test)\n",
        "\n",
        "    return {\"metrics\": metrics, \"train_zip\": str(z_train), \"test_zip\": str(z_test)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-19T01:37:10.607569Z",
          "iopub.status.busy": "2026-01-19T01:37:10.606756Z",
          "iopub.status.idle": "2026-01-19T01:37:19.035465Z",
          "shell.execute_reply": "2026-01-19T01:37:19.031797Z",
          "shell.execute_reply.started": "2026-01-19T01:37:10.607522Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 67.2MB/s 0.1s\n",
            "Ultralytics 8.4.6 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/ffb_localization_rgbd_train.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=exp_a3_rgbd_seed422, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/exp_a3_rgbd_seed422, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       608  ultralytics.nn.modules.conv.Conv             [4, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, 16, None, [64, 128, 256]] \n",
            "YOLO11n summary: 182 layers, 2,590,179 parameters, 2,590,163 gradients, 6.5 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "train: Fast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4368.5¬±1677.3 MB/s, size: 2705.1 KB)\n",
            "\u001b[Ktrain: Scanning /kaggle/working/ffb_localization_rgbd/rgb/train... 0 images, 280 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 280/280 68.1it/s 4.1s0.0s\n",
            "WARNING ‚ö†Ô∏è train: No labels found in /kaggle/working/ffb_localization_rgbd/rgb/train.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "train: New cache created: /kaggle/working/ffb_localization_rgbd/rgb/train.cache\n",
            "WARNING ‚ö†Ô∏è Labels are missing or empty in /kaggle/working/ffb_localization_rgbd/rgb/train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "val: Fast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 3061.2¬±2417.9 MB/s, size: 2705.1 KB)\n",
            "\u001b[Kval: Scanning /kaggle/working/ffb_localization_rgbd/rgb/val... 0 images, 80 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 80/80 70.3it/s 1.1s0.0s\n",
            "WARNING ‚ö†Ô∏è val: No labels found in /kaggle/working/ffb_localization_rgbd/rgb/val.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "val: New cache created: /kaggle/working/ffb_localization_rgbd/rgb/val.cache\n",
            "WARNING ‚ö†Ô∏è Labels are missing or empty in /kaggle/working/ffb_localization_rgbd/rgb/val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "Plotting labels to /kaggle/working/runs/detect/exp_a3_rgbd_seed422/labels.jpg... \n",
            "WARNING ‚ö†Ô∏è zero-size array to reduction operation maximum which has no identity\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.002, momentum=0.9) with parameter groups 0 weight(decay=0.0), 0 weight(decay=0.0005), 12 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/kaggle/working/runs/detect/exp_a3_rgbd_seed422\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/18  0.0s\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "out_seed42 = run_seed(42)\n",
        "out_seed123 = run_seed(123)\n",
        "\n",
        "print(\"Seed 42 metrics:\", out_seed42[\"metrics\"])\n",
        "print(\"Seed 42 zips:\", out_seed42[\"train_zip\"], out_seed42[\"test_zip\"])\n",
        "\n",
        "print(\"Seed 123 metrics:\", out_seed123[\"metrics\"])\n",
        "print(\"Seed 123 zips:\", out_seed123[\"train_zip\"], out_seed123[\"test_zip\"])\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 9288121,
          "sourceId": 14542234,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
